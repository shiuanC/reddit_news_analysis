[
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "Appendix"
  },
  {
    "objectID": "summary.html#about-the-team",
    "href": "summary.html#about-the-team",
    "title": "Introduction",
    "section": "About the Team",
    "text": "About the Team\n\nLucienne L. Julian      Sonali Subbu Rathinam   Peijin Li"
  },
  {
    "objectID": "goals.html",
    "href": "goals.html",
    "title": "Business Goals",
    "section": "",
    "text": "Business Goal -1 : Determine which day people use Reddit most frequently.\n\n Technical proposal:  To find this, we will extract the day from the DateTime variable. We will then group by the new day variable and count the group by data. We will create a bar graph of the counts to visualize the findings. This will be conducted for both comments and submissions.\n\n\n \n\n Business Goal - 2 : Determine which fake news identifier is used the most. \n\nTechnical Proposal : First, we will utilize regex to match on different fake news identifiers like bullshit, propaganda, or fake news. Using this, we will create a fake news indicator column that shows if the phrase was present in the comment. Then, we will group by the indicator column and count the groups. We will visualize our findings in a table.\n\n\n \n\n Business Goal - 3 : Determine the difference in the number of posts by year.\n\nTechnical proposal : We will extract the year from the DateTime variable. Using this, we will group by year and count the grouped data. We will visualize our findings in a table that includes comments and submissions.\n\n\n \n\nBusiness Goal - 4 : Who are those most active accounts in these news subreddits? \n\nTechnical Proposal : To determine the most active accounts in the news subreddits, we will identify the top 10 users with the highest number of posts. We will extract the usernames from the dataset and group by each user to count their posts. Then, we will compare the number of posts made by these top users in both subreddits. Finally, we will present the findings in a bar chart.\n\n\n \n\nBusiness Goal - 5 : Determine the frequency and the accumulated scores of news sources. \n\nTechnical Proposal:  In this analysis, our objective is to comprehend the frequency and cumulative scores associated with different news sources. Initially, we’ll extract the “domain” information from the URL of each post. Subsequently, we’ll aggregate the data based on these sources, computing both the frequency of posts and summarizing the scores attributed to each source.\n\n\n \n\n\n\n\n\nBusiness Goal - 6 : Determine the news topics that are interacted with the most.  Among all types of news, which category do people concern about the most? Is it related to war, disaster, food crisis or something else? \n\nTechnical Proposal: Use topic modeling to create different broad news categories. Group by those categories and analyze the comments.\n\n\n \n\nBusiness Goal - 7 : \nDetermine what topics are perceived as fake news. \n\nTechnical Proposal : Use topic modeling created above. Perform sentiment analysis on the comments. Comments with negative sentiment and containing misinformation identifiers will be misinformation. Topics with more of these comments will be perceived as fake news.\n\n\n \n\nBusiness Goal - 8 : \nFind this difference in perceived misinformation from 2021 (post-election year) to 2023 (pre-election year). \n\nTechnical Proposal : We will use the combined sentiment analysis and misinformation indicator to identify comments with negative sentiment and contain a misinformation indicator. We will group by this new variable and count the grouped data. Since there are significantly fewer comments in 2023, we will then take the rate of misinformation comments to compare the two.\n\n\n \n\n\n\n\nBusiness Goal - 9 : \nWhich articles might be perceived as misinformation, or labeled by users as fake news? \n\nTechnical Proposal : We will employ a supervised machine learning model to predict articles that are prone to be perceived as misinformation. Specifically, we’ll utilize techniques such as decision trees to discern which features contribute significantly to the classification process. By analyzing various attributes of article titles, we’ll determine patterns indicating potential fake news. This comprehensive approach will shed light on both the types of titles more likely to be labeled as fake news and the key features crucial for classification accuracy.\n\n\n \n\nBusiness Goal - 10 : \nAmong political topics, is there any correlation? Are certain topics mentioned together more frequently than others?\n\nTechnical Proposal : We will conduct a network analysis to explore the relationships between different political topics. This analysis will help us understand the connections and interactions among various political themes, providing insights into how they are interconnected or influence each other within the broader political landscape.",
    "crumbs": [
      "I. Overview",
      "Business Goals"
    ]
  },
  {
    "objectID": "goals.html#exploratory-data-analysis-goals",
    "href": "goals.html#exploratory-data-analysis-goals",
    "title": "Business Goals",
    "section": "",
    "text": "Business Goal -1 : Determine which day people use Reddit most frequently.\n\n Technical proposal:  To find this, we will extract the day from the DateTime variable. We will then group by the new day variable and count the group by data. We will create a bar graph of the counts to visualize the findings. This will be conducted for both comments and submissions.\n\n\n \n\n Business Goal - 2 : Determine which fake news identifier is used the most. \n\nTechnical Proposal : First, we will utilize regex to match on different fake news identifiers like bullshit, propaganda, or fake news. Using this, we will create a fake news indicator column that shows if the phrase was present in the comment. Then, we will group by the indicator column and count the groups. We will visualize our findings in a table.\n\n\n \n\n Business Goal - 3 : Determine the difference in the number of posts by year.\n\nTechnical proposal : We will extract the year from the DateTime variable. Using this, we will group by year and count the grouped data. We will visualize our findings in a table that includes comments and submissions.\n\n\n \n\nBusiness Goal - 4 : Who are those most active accounts in these news subreddits? \n\nTechnical Proposal : To determine the most active accounts in the news subreddits, we will identify the top 10 users with the highest number of posts. We will extract the usernames from the dataset and group by each user to count their posts. Then, we will compare the number of posts made by these top users in both subreddits. Finally, we will present the findings in a bar chart.\n\n\n \n\nBusiness Goal - 5 : Determine the frequency and the accumulated scores of news sources. \n\nTechnical Proposal:  In this analysis, our objective is to comprehend the frequency and cumulative scores associated with different news sources. Initially, we’ll extract the “domain” information from the URL of each post. Subsequently, we’ll aggregate the data based on these sources, computing both the frequency of posts and summarizing the scores attributed to each source.",
    "crumbs": [
      "I. Overview",
      "Business Goals"
    ]
  },
  {
    "objectID": "goals.html#natural-language-processing-goals",
    "href": "goals.html#natural-language-processing-goals",
    "title": "Business Goals",
    "section": "",
    "text": "Business Goal - 6 : Determine the news topics that are interacted with the most.  Among all types of news, which category do people concern about the most? Is it related to war, disaster, food crisis or something else? \n\nTechnical Proposal: Use topic modeling to create different broad news categories. Group by those categories and analyze the comments.\n\n\n \n\nBusiness Goal - 7 : \nDetermine what topics are perceived as fake news. \n\nTechnical Proposal : Use topic modeling created above. Perform sentiment analysis on the comments. Comments with negative sentiment and containing misinformation identifiers will be misinformation. Topics with more of these comments will be perceived as fake news.\n\n\n \n\nBusiness Goal - 8 : \nFind this difference in perceived misinformation from 2021 (post-election year) to 2023 (pre-election year). \n\nTechnical Proposal : We will use the combined sentiment analysis and misinformation indicator to identify comments with negative sentiment and contain a misinformation indicator. We will group by this new variable and count the grouped data. Since there are significantly fewer comments in 2023, we will then take the rate of misinformation comments to compare the two.",
    "crumbs": [
      "I. Overview",
      "Business Goals"
    ]
  },
  {
    "objectID": "goals.html#machine-learning-goals",
    "href": "goals.html#machine-learning-goals",
    "title": "Business Goals",
    "section": "",
    "text": "Business Goal - 9 : \nWhich articles might be perceived as misinformation, or labeled by users as fake news? \n\nTechnical Proposal : We will employ a supervised machine learning model to predict articles that are prone to be perceived as misinformation. Specifically, we’ll utilize techniques such as decision trees to discern which features contribute significantly to the classification process. By analyzing various attributes of article titles, we’ll determine patterns indicating potential fake news. This comprehensive approach will shed light on both the types of titles more likely to be labeled as fake news and the key features crucial for classification accuracy.\n\n\n \n\nBusiness Goal - 10 : \nAmong political topics, is there any correlation? Are certain topics mentioned together more frequently than others?\n\nTechnical Proposal : We will conduct a network analysis to explore the relationships between different political topics. This analysis will help us understand the connections and interactions among various political themes, providing insights into how they are interconnected or influence each other within the broader political landscape.",
    "crumbs": [
      "I. Overview",
      "Business Goals"
    ]
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "We assessed the basic specifications of the dataset, removed duplicates and anomalies and dropped undesired columns to finally get 170,144 submissions and 18,548,934 comments in the respective datasets. The variables of interest after preprocessing the datasets are listed below:\nIn the  Submissions  dataset :\n\n\n\nauthor : The user who created the post.\n\n\ncreated_utc : The time the submission or comment was posted. Was used in the time series analysis section of our project.\n\n\ndomain : The news site posted in the submission.\n\n\nid : The unique identifier of each post.\n\n\nnum_comments : The number of comments under each submission. This may not capture the exact picture as it is dependent on the day the data was retrieved.\n\n\nurl : The url associated with the post. Most of the submissions contain this url to the news article.\n\n\nscore : The Karma score awarded to each post\n\n\nIn the  Comments  dataset :\n\n\n\nauthor : The user who posted the comment.\n\n\ncreated_utc : The time the submission or comment was posted. Was used in the time series analysis section of our project.\n\n\nbody : The text in the comment.\n\n\n id : The unique identifier of each comment.\n\n\nlink_id : The id of the submission under which the comment exists.\n\n\ncontroversiality : Whether a comment was classified as ‘controversial’.\n\n\n gilded : Whether the comments have been gilded or awarded.\n\n\n distinguished : Whether the comments have been distinguished as moderator.\n\n\n score : The Karma score awarded to each post\n\n\nDuring this process, several dummy variables were created to aid in our analysis. A foreign key like variable called submission_id was also created in the comments dataset, that linked any comment to the submission it was made under.",
    "crumbs": [
      "Exploratory Data Analysis",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#preprocessing-and-data-dictionary",
    "href": "eda.html#preprocessing-and-data-dictionary",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "We assessed the basic specifications of the dataset, removed duplicates and anomalies and dropped undesired columns to finally get 170,144 submissions and 18,548,934 comments in the respective datasets. The variables of interest after preprocessing the datasets are listed below:\nIn the  Submissions  dataset :\n\n\n\nauthor : The user who created the post.\n\n\ncreated_utc : The time the submission or comment was posted. Was used in the time series analysis section of our project.\n\n\ndomain : The news site posted in the submission.\n\n\nid : The unique identifier of each post.\n\n\nnum_comments : The number of comments under each submission. This may not capture the exact picture as it is dependent on the day the data was retrieved.\n\n\nurl : The url associated with the post. Most of the submissions contain this url to the news article.\n\n\nscore : The Karma score awarded to each post\n\n\nIn the  Comments  dataset :\n\n\n\nauthor : The user who posted the comment.\n\n\ncreated_utc : The time the submission or comment was posted. Was used in the time series analysis section of our project.\n\n\nbody : The text in the comment.\n\n\n id : The unique identifier of each comment.\n\n\nlink_id : The id of the submission under which the comment exists.\n\n\ncontroversiality : Whether a comment was classified as ‘controversial’.\n\n\n gilded : Whether the comments have been gilded or awarded.\n\n\n distinguished : Whether the comments have been distinguished as moderator.\n\n\n score : The Karma score awarded to each post\n\n\nDuring this process, several dummy variables were created to aid in our analysis. A foreign key like variable called submission_id was also created in the comments dataset, that linked any comment to the submission it was made under.",
    "crumbs": [
      "Exploratory Data Analysis",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#analyzing-weekly-activity-trends",
    "href": "eda.html#analyzing-weekly-activity-trends",
    "title": "Exploratory Data Analysis",
    "section": "Analyzing Weekly Activity Trends",
    "text": "Analyzing Weekly Activity Trends\nIn this section, we delve into understanding the patterns of user engagement on Reddit. We begin by examining the distribution of activity across different days of the week, extracted from the DateTime variable. By grouping the data based on these days, we quantify the frequency of user interactions, both in terms of comments and submissions. The insights derived from this analysis are visually represented through a bar graph in Figure 2.1 and Figure 2.2.\n\n\n\n\n\n\n\n\n\n Figure 2.1: Number of submissions per day of the week from 2021-2023   As shown in Figure 2.1, the trend for submissions exhibits a significant dip during the weekends. This suggests that users are less likely to initiate new threads or topics on Saturday and Sunday.A potential factor contributing to this weekend slump could be the downtime in news cycles, as journalists and news outlets typically slow down on these days.Interestingly, Thursday is the busiest day for submissions, contrary to the intuitive expectation that Monday would start the week with a surge.\n\n\n\n\n\n\n\n\n\n Figure 2.2: Number of comments per day of the week from 2021-2023   Figure 2.2 depicts a different dynamic for comments, with activity gradually increasing from Monday, reaching a zenith on Thursday. This progressive increase could indicate users’ growing engagement with content as the week unfolds. Despite both submissions and comments peaking on Thursday, only comments display a steady climb throughout the weekdays.  When we synthesize the data from submissions and comments, a compelling narrative about user engagement emerges. Thursday stands out as a pinnacle of activity for Reddit, with both submissions and comments reaching their highest levels. This indicates that Thursdays are not just about new content being created but also about the peak in interactions with existing threads. The pattern across the week shows more engagement with ongoing discussions rather than starting new ones, especially as the week progresses.",
    "crumbs": [
      "Exploratory Data Analysis",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#analyzing-post-frequency-by-year",
    "href": "eda.html#analyzing-post-frequency-by-year",
    "title": "Exploratory Data Analysis",
    "section": "Analyzing Post Frequency by Year",
    "text": "Analyzing Post Frequency by Year\nIn this analysis, we investigate the disparity in the number of posts across different years. Utilizing the DateTime variable, we extract the year component to group the data accordingly. The findings are presented in two separate tables: Table 2.1 for submissions and Table 2.2 for comments.\n\n\n╒════════╤═════════╕\n│   year │   count │\n╞════════╪═════════╡\n│   2021 │  998335 │\n│   2022 │  732024 │\n│   2023 │  116779 │\n╘════════╧═════════╛\n\n\n Table 2.1: Submissions Per Year  \n\n\n╒════════╤══════════╕\n│   year │    count │\n╞════════╪══════════╡\n│   2022 │ 27272489 │\n│   2021 │ 19060400 │\n│   2023 │  3637011 │\n╘════════╧══════════╛\n\n\n Table 2.2: Comments Per Year   The analysis of post frequency by year reveals intriguing trends. While there is a notable decline in submissions from 2021 to 2023, the number of comments exhibits a contrasting pattern, with a significant increase observed from 2021 to 2022 followed by a decline in 2023. This divergence suggests a potential shift in user behavior towards increased engagement with existing content rather than generating new posts. Further investigation into the underlying factors influencing this trend could provide valuable insights into evolving user preferences and platform dynamics.",
    "crumbs": [
      "Exploratory Data Analysis",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#assessing-percentage-of-comments-with-fake-news-indicators",
    "href": "eda.html#assessing-percentage-of-comments-with-fake-news-indicators",
    "title": "Exploratory Data Analysis",
    "section": "Assessing Percentage of Comments with Fake News Indicators",
    "text": "Assessing Percentage of Comments with Fake News Indicators\nTo evaluate the prevalence of fake news indicators in comments, we employed regex to detect phrases such as “fake news,” “bullshit,” or “propaganda.” Subsequently, a fake news indicator column was created to denote the presence of these phrases in comments. Grouping the data by this indicator column, we tallied the counts and visualized the findings in a chart. Among the total comments analyzed, 407,621 were flagged as containing fake news indicators, while the majority, comprising 49,562,279 comments, were deemed free from such indicators. This signifies that approximately 0.8% of comments were identified as potentially containing fake news elements. Further exploration into the context and implications of these comments could offer valuable insights into the dissemination of misinformation within online communities.\n\n\n\n\n\n\n\n\n\n Figure 2.3: Comments containing Misinformation Indicators from 2021-2023",
    "crumbs": [
      "Exploratory Data Analysis",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#analyzing-user-activity",
    "href": "eda.html#analyzing-user-activity",
    "title": "Exploratory Data Analysis",
    "section": "Analyzing User Activity",
    "text": "Analyzing User Activity\nAs we delve into the dynamics of user interactions within the news-centric communities on Reddit, we encounter some intriguing patterns. The subreddit under examination boasts 31.5 million subscribers. However, a closer inspection reveals that in the past year, approximately 27,000 unique users have made submissions, and around 1.2 million have commented.\n\n\n\n\n\n\n\n\n\n Figure 2.4: Top 10 Users Post Comparison for News and World News Subreddits   A minute fraction of these users contribute the bulk of the submissions, which is delineated in Figure 2.4. This chart illustrates a comparison of posting frequency among the top 10 users in both the News and World News subreddits. The disparity is stark; the top-ranked user alone is responsible for more than 60,000 submissions in the News subreddit, significantly overshadowing the rest.",
    "crumbs": [
      "Exploratory Data Analysis",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#source-analysis",
    "href": "eda.html#source-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Source Analysis",
    "text": "Source Analysis\nIn this extensive analysis, we aim to understand the influence of various news sources within the Reddit ecosystem. To achieve this, we extract the domain information from each submission’s URL and aggregate the data to evaluate two main metrics: the frequency of posts from each news source and the cumulative karma scores these posts have received.\nThis dual-faceted approach enables us to not only see which news sources are most frequently posted but also which ones resonate the most with the Reddit community, as reflected by their karma scores. The results are encapsulated in two distinct tables.\n\n\n\n\n\n\n\n\n\n Figure 2.5: Top 10 Users Post Comparison for News and World News Subreddits   To visually represent the frequency of posts by these news sources, we created a word cloud, as shown in the uploaded image. This illustration vividly displays the prominence of certain names, with ‘Reuters’ and ‘BBCWorld’ appearing more prominently, indicating a higher frequency of posts from these sources compared to others like ‘rajacreator’ or ‘tellygupshup’.\n\n\n╒══════════════╤═════════════╤═══════════════╤══════════════╕\n│ source       │   frequency │   karma_score │   score rank │\n╞══════════════╪═════════════╪═══════════════╪══════════════╡\n│ Reuters      │      319453 │        327004 │           43 │\n│ rajacreator  │       68753 │         68753 │           45 │\n│ AP           │       63886 │         67112 │           46 │\n│ bluzz        │       57339 │         57337 │           47 │\n│ reuters      │       39747 │      34230489 │            1 │\n│ youtube      │       38580 │         38472 │           48 │\n│ news         │       34528 │      13992972 │           10 │\n│ BBCWorld     │       33884 │         34733 │           49 │\n│ tellygupshup │       31600 │         31596 │           50 │\n│ popularnews  │       30492 │         30491 │           51 │\n╘══════════════╧═════════════╧═══════════════╧══════════════╛\n\n\n Table 2.3: Top 10 Sources by Frequency  \nTable 2.3 lists the top 10 sources by the frequency of posts. Reuters takes the lead in the number of posts, with a significant presence on the platform. However, when we consider the karma scores, the same source shows a dramatic contrast in its two entries, one with a high karma score and another much lower, indicating perhaps a discrepancy in the content’s reception or the presence of multiple accounts associated with the source.\n\n\n╒═════════════════╤═════════════╤═══════════════╤══════════════════╕\n│ source          │   frequency │ karma_score   │   frequency rank │\n╞═════════════════╪═════════════╪═══════════════╪══════════════════╡\n│ reuters         │       39747 │ 34,230,489    │                5 │\n│ cnn             │       20535 │ 26,569,966    │               13 │\n│ theguardian     │       24648 │ 24,930,338    │               12 │\n│ apnews          │       20407 │ 23,547,749    │               14 │\n│ nbcnews         │       10907 │ 20,146,178    │               21 │\n│ cnbc            │        7391 │ 16,038,330    │               31 │\n│ businessinsider │        6772 │ 15,497,047    │               33 │\n│ bbc             │       29034 │ 15,373,615    │               11 │\n│ cbsnews         │        5514 │ 14,944,943    │               41 │\n│ news            │       34528 │ 13,992,972    │                7 │\n╘═════════════════╧═════════════╧═══════════════╧══════════════════╛\n\n\n Table 2.4: Top 10 Sources by Karma score   Table 2.4, on the other hand, ranks the top 10 sources by the karma score. This table reveals that while Reuters may not have the highest posting frequency among the top sources, it garners the highest cumulative karma, suggesting a strong engagement from the Reddit community with the content provided by this source.  These analyses not only highlight the most active news sources but also offer insights into the quality of engagement that different sources inspire among Redditors. Such data is invaluable for understanding the landscape of news consumption and dissemination on one of the world’s largest social platforms.",
    "crumbs": [
      "Exploratory Data Analysis",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Conclusion",
    "section": "",
    "text": "This project aimed to analyze the comments and posts on the r/worldnews subreddit through natural language processing (NLP) and machine learning (ML) techniques. In our Exploratory Data Analysis (EDA), we identified what news sites were primarily shared on the subreddit, via aggregated submission counts; began seeing what stories were most popular, based on karma scores; found that the discussion threads were heavily tied to the Russia-Ukraine Conflict; and examined the distribution of comments and submissions over time. We noted that several of the shared sites were American in origin, but also saw that many of the popular sources came from several European countries, including Russia. Other social media sites were also a popular submission choice, especially YouTube. Using the ACLED data, we were also able to determine that while the conflict was receiving a large share of attention, not all of the events of the conflict were being shared equally. Additionally, the time series analysis saw that there were multiple gaps in the dataset for certain time periods.\n Table 5.1 : Most Shared News Sites \nIn the NLP stage of our analysis, we chose to look closer at the topics and entities that the submissions were about, as well as examine the sentiment of both the comments and submissions. In our topic modelling we found that using the whole data resulted in groups with substantial amounts of overlap regarding the conflict. We then constrained the data to time periods that saw high amounts of search interest in Google Trends, creating two subsets which included posts and comments that occurred up to two weeks after the designated event’s date. The resulting subsetted data produced clearer topic groups, with specific events being highlighted, although the cluster of groups which were hard to differentiate remained. For the sentiment analysis at this stage, we used pretrained models from JohnSnowLabs to examine both the sentiment of the comments in the threads as well as comments and submission titles related to the Russia-Ukraine Conflict. Of the three different models used, Twitter and IMDB had identical results while the Vivek model registered a larger portion of the comments and submissions as neutral, but all three models indicated negative sentiment as the plurality of sentiments, if not the majority. The live threads’ results indicated strong sentiments, both positive and negative across the comments, a pattern which seemed to hold in the comments of submissions. The article titles, however, demonstrate much greater negativity according to the IMDB and Twitter models, while the Vivek model read it as again having more neutrality.\n Table 5.2 : Pre-Trained Models Sentiment Results on Comments and Submissions \nLastly, we have successfully designed and implemented predictive ML models to predict controversiality markers and the sentiment of comments and submissions. The controversiality model took in a TF-IDF weighted vector of the comments and was able to achieve a high level of predictive strength, although that seemed to be a result, in large part, due to the class imbalance. We believe that the deleted and removed comments were likely controversial, and so the remaining controversial ones were not being predicted as well as we would hope. For the sentiment model we decided to use Vader-Sentiment lexicon to label our data rather than relying exclusively on pretrained models. The dataset we used to classify sentiment appeared to label more similarly to the Vivek model, with the plurality being negative but showing a larger proportion of neutral material. Several ML algorithms were then used to train the labelled dataset and we ultimately saw great success with test error at slightly above 8%.\n Figure 5.1 : ROC Curves for Different Predictive Models Used in Predicting Controversiality \nMoving forward, we feel that we can further improve our models via the use of over and under sampling techniques to deal with the class imbalance. Additionally, we believe that we can improve on a predictive model for karma scores that we attempted, although at present its accuracy can be described only as pitiful."
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "Natural Language Processing Analysis",
    "section": "",
    "text": "In this round of analysis we worked with three  Natural Language Processing (NLP)  techniques namely\n\n\nTopic Modeling\n\n\nNamed Entity Recognition (NER)\n\n\nSentiment Analysis\n\n\nOur results from both Topic Modeling and NER indicate widespread consumption of news on the ongoing Russia-Ukraine Conflict. In both cases we saw that the topics our model saw and the entities being referenced most often were either directly involved or adjacent to said conflict. We did see that when subsetting the data around events of the conflict, the topics our models produce had less overlap. Lastly, our Sentiment Analysis on submissions and comments related to the conflict suggested that submissions tended to be more negative while comments had an equal distribution of both negative and positive sentiments. The models appeared to demonstrate low neutrality in both submissions and comments."
  },
  {
    "objectID": "nlp.html#executive-summary",
    "href": "nlp.html#executive-summary",
    "title": "Natural Language Processing Analysis",
    "section": "",
    "text": "In this round of analysis we worked with three  Natural Language Processing (NLP)  techniques namely\n\n\nTopic Modeling\n\n\nNamed Entity Recognition (NER)\n\n\nSentiment Analysis\n\n\nOur results from both Topic Modeling and NER indicate widespread consumption of news on the ongoing Russia-Ukraine Conflict. In both cases we saw that the topics our model saw and the entities being referenced most often were either directly involved or adjacent to said conflict. We did see that when subsetting the data around events of the conflict, the topics our models produce had less overlap. Lastly, our Sentiment Analysis on submissions and comments related to the conflict suggested that submissions tended to be more negative while comments had an equal distribution of both negative and positive sentiments. The models appeared to demonstrate low neutrality in both submissions and comments."
  },
  {
    "objectID": "nlp.html#text-pre-processing",
    "href": "nlp.html#text-pre-processing",
    "title": "Natural Language Processing Analysis",
    "section": "Text Pre-Processing",
    "text": "Text Pre-Processing\nWe started off our analysis by assessing the number of words in the title and body columns of our datasets. They are represented in the two graphs below.\nBoth these graphs in figures 1 and 2 are right skewed, reflecting that a major amount of both titles and comments are below 50 words.\nNext, we moved on to preprocessing. In order to do the natural language processing (NLP) tasks, the corpuses of comments and submissions needed to face several steps of processing as listed below:\n\n\nRemove stop words\n\n\nRemove other languages\n\n\nRemove special characters\n\n\nConvert text to lower case\n\n\nLemmatize words\n\n\nThe above listed preprocessing steps were applied to text portions of our submissions and comments datasets, on the columns title and body respectively. We used the johnsnowlabs package to conduct our preprocessing and implemented our preprocessing task through a pipeline. systematically converted to document file types and then tokenized. This transformed the data into a bag of words form and was used to accomplish the business goals below.\nAfter the preprocessing, we assessed the 15 most commonly occurring words in both the submissions and comments dataset as displayed in table 1.\n Table 3.1 : Most Common Words in the Submissions Datasets \n Table 3.2 : Most Common Words in the Comments Datasets \nIt is evident from both tables 3.1 and 3.2 that the Russia-Ukraine Conflict dominates both in the submissions and comments datasets. This result agrees with our EDA analysis. Even when constructing a Word Cloud with the Submissions dataset, we found similar trends as displayed in figure 3.3 below.\n\nFigure 3.3 : Word Cloud of Submissions Dataset\n\n\n Digging in further, we also evaluated the most important words using the Term Frequency - Inverse Document Frequency (TF-IDF)  methodology and obtained the following results.\n Table 3.3 : Most Important Words in the Datasets \nThe TF-IDF doesn’t reflect the dominance of war related submissions and comments obtained above. This will be further explored through our following analysis of dominant topics in the data."
  },
  {
    "objectID": "nlp.html#key-topics-in-submissions",
    "href": "nlp.html#key-topics-in-submissions",
    "title": "Natural Language Processing Analysis",
    "section": "Key Topics in Submissions",
    "text": "Key Topics in Submissions\nWe employed topic modeling through Latent Dirchlet Allocation (LDA) on the cleaned titles of our submissions dataset and obtained 7 topic groups, which saw most of the groups dealing with the Russia-Ukraine Conflict, addressing its different facets. The topics were:\n\n\nTopic 1: deals with the Russia-Ukraine Conflict and nuclear weapons, power plants\n\n\nTopic 2: deals with the Russia-Ukraine Conflict and the media/news coverage on the political leaders\n\n\nTopic 3: deals with the Russia-Ukraine Conflict and oil imports, the EU, laws, and regulation\n\n\nTopic 4: deals with the Russia-Ukraine Conflict as well as Twitter and Elon Musk\n\n\nTopic 5: news related to covid, China, and Korea\n\n\nTopic 6: deals with the Russia-Ukraine Conflict and neighbouring countries\n\n\nTopic 7: the Russia-Ukraine Conflict and the Qatar World Cup\n\n\nAs stated, several of these topics related to the Russia-Ukraine Conflict directly. This high centralization of topic matter seemed to affect topics 4 and 7 which appeared to combine disparate subjects. The LDA topic visualization is displayed below.\n\nFigure 3.4 : LDA Topic Visualization for Submissions Datset\n\nFurther viewing the , utilizing google trends data we were able to determine events in the conflict which most affected internet traffic. Using this we sought to examine how such key events affected the subreddit’s submissions, ie news coverage. The first event selected was the start of the war, which we used to create a subset of the data that encompassed the first two weeks of the Russia-Ukraine Conflict and was modeled with only a topic count of 5. Its results saw the following topics:\n\n\nTopics 1, 2, 3, 5 all relating to the Russia-Ukraine Conflict, with repeated terms (political leaders of both countries, cities in both countries, and weapons - missiles, nuclear, military, troops)\n\n\nTopic 4 provided an interesting insight - it includes news related to Indian students in Ukraine, as well as Starlink and Elon Musk.\n\n\nTo provide context, topic 4 highlighted the early involvement of Musk in providing his satellite services to the country and the Nazi Azov battalion accosting foreign students attempting to flee.\n\nFigure 3.5 : LDA Topic Visualization for Submissions Datset During the Start of the War\n\n The second event we used from the trends coverage, was when Russia began withdrawing. After applying LDA once more, we saw the topic modelling results indicated:\n\n\nTopics 1, 2, 3, 4 all relating to Russia-Ukraine Conflict, with repeated terms (political leaders of both countries, cities in both countries, and weapons - missiles, nuclear, military, troops)\n\n\nTopic 5 : news related to North Korea firing a ballistic missile over Japan.\n\n\nAs was stated, North Korea had fired a ballistic missile over Japan in the same time period and had been rapidly increasing the rate of testing. This appears to suggest that the smaller subsets of data might be more useful in capturing specific stories without overlapping the conflict with other topics.\n\nFigure 3.6 : LDA Topic Visualization for Submissions Datset When Russian Troops Started Receding"
  },
  {
    "objectID": "nlp.html#identifying-key-entities-in-the-datasets",
    "href": "nlp.html#identifying-key-entities-in-the-datasets",
    "title": "Natural Language Processing Analysis",
    "section": "Identifying Key Entities in the Datasets",
    "text": "Identifying Key Entities in the Datasets\nThe top 12 entities for person, location and organization are displayed in table 3. We used BERT  pretrained  Named Entity Recognition (NER)  models to accomplish this. Initially, when feeding the pre trained model with our pre-processed data, we found that Russia and Ukraine didn’t feature as one of the top entities. Upon re-running the model pipeline with unprocessed data, the results were more familiar.\nGiven the density of topics demonstrated by the previous section’s analysis, the focus of the entities on the conflict should not come as a surprise. Using NER, we saw that just as the topics were heavily saturated with the Russia-Ukraine Conflict, the results for most frequently mentioned entities demonstrated this as well.\nIn table 3.4, we can clearly see the prevalence of Putin, Biden, and Zelensky; but these persons are not the only noteworthy results. We also can see British political figures, Boris Johnson and Liz Truss, taking a prominent role in the results. Considering the domain table from our EDA, British news coverage made up some of the most popular sources for posts. This could be the reason for the British political figures being highlighted by the NER model.\nWhen examining the locations results, we saw Ukraine and Russia occur with much greater frequency than the other states, followed by the United States, which appeared twice, and China to a lesser extent.\nLastly our organization based model appeared to return the most mixed results. The top three results were organizations that could easily indicate stories related to the Russia-Ukraine Conflict, but after this the results become less than ideal. Ukrainian cities, Russia, Covid, and Elon Musk were included in the results. This could be seen as a short coming of the models, but also testifies to the prevalence of the conflict in the subreddits submissions. It should also be noted that different n-grams and misspellings resulted in less refined results, and, as stated, the organization results were rather mixed. Table 3.4 displays the entities in the decreasing order of occurence.\n Table 3.4 : Top Entities in Submissions"
  },
  {
    "objectID": "nlp.html#assessing-sentiments-on-war-related-comments-and-submissions",
    "href": "nlp.html#assessing-sentiments-on-war-related-comments-and-submissions",
    "title": "Natural Language Processing Analysis",
    "section": "Assessing Sentiments on War Related Comments and Submissions",
    "text": "Assessing Sentiments on War Related Comments and Submissions\nBased on our previous analysis in the EDA, it has been determined that the live threads in our subreddit were predisposed towards the Russia-Ukraine Conflict. The dominance of said conflict in the submissions was further supported by topic modeling results. We expected that such a topic would lead itself towards strong sentiments and were affirmed in that expectation through employing pre-trained sentiment models as shown in table 3.5 below.\n Table 3.5 : Sentiment Analysis Results From Live Thread Comments \nAlthough the Vivek analyzer categorized more comments as neutral, overall users appeared to have stronger sentiments, generally of a negative sort. This negative pattern was demonstrated to an equal extent by both the IMDB and Twitter models, without significant difference.\nFurther sentiment analysis was done on submissions related to the Russia-Ukraine Conflict and their related comments, excluding the above analyzed live threads. The models appear to have similar trends, but it should be noted that that for the Twitter and IMDB models the submission titles, those of the articles being shared, were significantly more negative than the comments. Overall the title results for each model indicated 25.22% more overall negative sentiment than the related comment sentiment results, all of which can be seen in table 3.6 below.\n Table 3.6 : Sentiment Analysis of War-Related Submissions and Comments \nThe stark difference in results between the Vivek Model and the Twitter and IMDB models is more apparent here. We assumed that titles would have demonstrated more neutral sentiment, as the Vivek model demonstrated; but assuming that the results IMDB and Twitter models are more reliable, marketing tactics may be the culprit for the perceived negativity. Given how article titles have increasingly become the method of user interaction in social media, drawing attention with inflammatory language can be a tactic for increasing user interest. This perceived negativity might also be explained by the dominance of Western Media outlets in this subreddit, that was seen in the EDA analysis.\nWe attempted a lexicon based approach, but due to logistical issues we instead went forward with pretrained models  sentimentdl_use_twitter, sentimentdl_use_imdb and  VivekSentimentAnalyzer . In the future, we believe that training our own sentiment analyzer could improve the results."
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "Machine Learning Analysis",
    "section": "",
    "text": "For our Machine Learning (ML) Analysis we accomplished two separate tasks, one predicting comment sentiment and the second predicting comment controversy. We saw that an SVM model using cross validation was best at predicting comment sentiment, leading in all grading metrics we checked. This sentiment prediction model was saved following the results, with its training. For the models predicting comment controversy, we saw high overall accuracy, but that seemed to have been a result of class imbalance. We describe where the large imbalance between controversial and noncontroversial comments may have come from, and how it may have affected our results. \nAdditionally we attempted to implement a model outside of our business goals to see whether we would be able to predict karma scores. While we saw promising results for the first two, our karma scores models faced several issues and had what we considered ‘pitiful’ results, due to its low accuracy. We have chosen instead to include it in the further analysis section below."
  },
  {
    "objectID": "ml.html#executive-summary",
    "href": "ml.html#executive-summary",
    "title": "Machine Learning Analysis",
    "section": "",
    "text": "For our Machine Learning (ML) Analysis we accomplished two separate tasks, one predicting comment sentiment and the second predicting comment controversy. We saw that an SVM model using cross validation was best at predicting comment sentiment, leading in all grading metrics we checked. This sentiment prediction model was saved following the results, with its training. For the models predicting comment controversy, we saw high overall accuracy, but that seemed to have been a result of class imbalance. We describe where the large imbalance between controversial and noncontroversial comments may have come from, and how it may have affected our results. \nAdditionally we attempted to implement a model outside of our business goals to see whether we would be able to predict karma scores. While we saw promising results for the first two, our karma scores models faced several issues and had what we considered ‘pitiful’ results, due to its low accuracy. We have chosen instead to include it in the further analysis section below."
  },
  {
    "objectID": "ml.html#supervised-sentiment-learning",
    "href": "ml.html#supervised-sentiment-learning",
    "title": "Machine Learning Analysis",
    "section": "Supervised Sentiment Learning",
    "text": "Supervised Sentiment Learning\nFor training a sentiment classifier, we needed sentiment labels on part of our dataset. To generate these labels, we used the vaderSentiment python package and tweaked the cluster bootstrap script to install the packages on the driver nodes. A portion of the comments dataset (20%) was labelled using this Vader-Sentiment lexicon. These comments were then used to train a sentiment classifier using supervised learning models namely Logistic Regression, Multinomial Naive Bayes and Support Vector Machines (SVM). After applying 5-fold cross-validation and tuning hyperparameters, we obtained evaluation metrics for the three models as displayed below in table 1.\n Table 4.1 : Evaluation Metrics for Supervised Sentiment Models \nFrom table 4.1, it is evident that Support Vector Machines performed the best. This model was then saved and used to label the remaining 80% of the comments dataset. Figure 1 portrays the distribution of sentiments across the comments dataset.\nThis model classifies more comments as neutral as compared to the pre-trained models deployed in the Natural Language Processing analysis section. This is reflected in figure 2 where the sentiment distribution of live thread comments is presented.\nVader-Sentiment lexicon is optimized to work with tweets. Despite this, we observe no similarity with the pre-trained twitter model and the Vader-Sentiment model."
  },
  {
    "objectID": "ml.html#predicting-controversiality",
    "href": "ml.html#predicting-controversiality",
    "title": "Machine Learning Analysis",
    "section": "Predicting Controversiality",
    "text": "Predicting Controversiality\nTo accomplish this goal, we converted the text data to TF-IDF weighted vectors of the terms and used them as the input for a variety of models. Testing Logistic Regression, LinearSVC, and NaiveBayes models we sought to predict the controversiality in our comments, the results of which are listed below.\n Figure 4.3 : ROC Curves for Different Models \nAlthough the ROC curves would indicate that logistic regression was the best performing model, that may be a result of the class imbalance in controversiality. During our preprocessing steps we removed comments that were listed as ‘removed’ or ‘deleted’. We suspect that such comments removal likely affected the predictive capacity of our data set, compounding on the already existing class balance issues.\n Table 4.2 : Evaluation Metrics for Controversiality Prediction Models \nIn this table we see that both Logistic Regression and Linear SVC have seemingly perfect recall scores, a result of them almost entirely ignoring comments with controversiality  labeled as 1. This appears to confirm our supposition that the underlying class imbalance in our dataset has led to biased models. We believe that this could be further improved in future analysis through the institution of oversampling or undersampling methods or training models such as Complement Naive Bayes that account for class imbalances."
  },
  {
    "objectID": "ml.html#further-analysis-predicting-karma-scores",
    "href": "ml.html#further-analysis-predicting-karma-scores",
    "title": "Machine Learning Analysis",
    "section": "Further Analysis (Predicting Karma Scores)",
    "text": "Further Analysis (Predicting Karma Scores)\nWe also aimed at constructing a regression model to predict the score column in our comments dataset. For this purpose, we used a multitude of predictors such as sentiment_label (obtained from previously trained sentiment classifier), gilded, and distinguished to name a few. Our initial implementation of Linear Regression and Decision Tree models produced sub-optimal (R-squared value of 7%) results. We plan to further work on this task by tuning hyperparameters, assessing scaling requirements and incorporating additional predictors to obtain a well-performing regression model."
  }
]