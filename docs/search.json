[
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "Appendix"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Introduction",
    "section": "",
    "text": "In the past four years, it seems like misinformation claims have skyrocketed in the United States. With major events like national elections and the COVID-19 pandemic, it seems like people have become increasingly susceptible to false narratives and misinformation. This trend poses significant challenges to public discourse and individual decision-making, making it imperative to study and understand the mechanisms and impacts of misinformation. Our project focuses on analyzing misinformation claims on Reddit, one of the largest and most influential social media platforms. By concentrating our analysis on the “news” and “worldnews” subreddits from 2021 to 2023, we aim to uncover patterns and trends that characterize the spread and reception of misinformation during this period.\n\n\n\n\n\n\n\n\n\n Figure 1.1: News Sub-Reddit\n\n\n\n\n\n\n\n\n\n Figure 1.2: World News Sub-Reddit\nSubmissions in r/worldnews and r/news typically include the article’s title along with a link, encouraging users to review the source material before returning to Reddit to discuss the news story. This format fosters an environment where diverse viewpoints and insights can be shared, thereby enriching the discussion and enhancing user engagement. The interactive nature of these subreddits not only serves as a platform for news dissemination but also as a forum for vibrant community interactions, making them ideal for observing trends in information sharing and misinformation.\nTo enrich our analysis, we incorporated additional data from the Google COVID-19 Vaccination Data. TWe use its epidemic data including vaccine data, confirmed cases, and more to draw correlations between real-world events and trends in online conversations. This integration allows us to create a comprehensive timeline that aligns significant health events with spikes in user activity and sentiment. By juxtaposing these datasets, we aim to understand better how global events influence public opinion and information dissemination on digital platforms.\n\n\n                                                \n\n\n Figure 1.3: Monthly Log Frequecy of Comments in News and World News Sub-reddits from 2021 - 2023\n\n\n                                                \n\n\n Figure 1.4: Monthly Log Frequecy of Posts in News and World News Sub-reddits from 2021 - 2023\nFurthermore, this cross-referenced data approach provides a unique perspective on the direct and indirect effects of global crises on digital media consumption and user behavior. Our goal is to use this analysis to identify patterns that could help predict and mitigate the spread of misinformation in future scenarios, thereby contributing to more informed and resilient online communities.\nThis project was executed using Amazon Web Services (AWS) with Spark to manage the data efficiently. Our analytical approach involved a blend of models, notably using pretrained models from JohnSnowLabs and custom models developed using VaderSentiment analysis. All project code and documentation are accessible via the GitHub link provided on the banner at the bottom of each page.\nBy delving deep into the dynamics of subreddit discussions and utilizing advanced data-processing capabilities, we endeavor to pave the way for more sophisticated methods of identifying and combating misinformation. This research not only sheds light on the patterns of news consumption but also offers a blueprint for enhancing the efficacy of digital platforms as tools for truthful communication.",
    "crumbs": [
      "I. Overview",
      "Introduction"
    ]
  },
  {
    "objectID": "summary.html#background",
    "href": "summary.html#background",
    "title": "Introduction",
    "section": "",
    "text": "In the past four years, it seems like misinformation claims have skyrocketed in the United States. With major events like national elections and the COVID-19 pandemic, it seems like people have become increasingly susceptible to false narratives and misinformation. This trend poses significant challenges to public discourse and individual decision-making, making it imperative to study and understand the mechanisms and impacts of misinformation. Our project focuses on analyzing misinformation claims on Reddit, one of the largest and most influential social media platforms. By concentrating our analysis on the “news” and “worldnews” subreddits from 2021 to 2023, we aim to uncover patterns and trends that characterize the spread and reception of misinformation during this period.\n\n\n\n\n\n\n\n\n\n Figure 1.1: News Sub-Reddit\n\n\n\n\n\n\n\n\n\n Figure 1.2: World News Sub-Reddit\nSubmissions in r/worldnews and r/news typically include the article’s title along with a link, encouraging users to review the source material before returning to Reddit to discuss the news story. This format fosters an environment where diverse viewpoints and insights can be shared, thereby enriching the discussion and enhancing user engagement. The interactive nature of these subreddits not only serves as a platform for news dissemination but also as a forum for vibrant community interactions, making them ideal for observing trends in information sharing and misinformation.\nTo enrich our analysis, we incorporated additional data from the Google COVID-19 Vaccination Data. TWe use its epidemic data including vaccine data, confirmed cases, and more to draw correlations between real-world events and trends in online conversations. This integration allows us to create a comprehensive timeline that aligns significant health events with spikes in user activity and sentiment. By juxtaposing these datasets, we aim to understand better how global events influence public opinion and information dissemination on digital platforms.\n\n\n                                                \n\n\n Figure 1.3: Monthly Log Frequecy of Comments in News and World News Sub-reddits from 2021 - 2023\n\n\n                                                \n\n\n Figure 1.4: Monthly Log Frequecy of Posts in News and World News Sub-reddits from 2021 - 2023\nFurthermore, this cross-referenced data approach provides a unique perspective on the direct and indirect effects of global crises on digital media consumption and user behavior. Our goal is to use this analysis to identify patterns that could help predict and mitigate the spread of misinformation in future scenarios, thereby contributing to more informed and resilient online communities.\nThis project was executed using Amazon Web Services (AWS) with Spark to manage the data efficiently. Our analytical approach involved a blend of models, notably using pretrained models from JohnSnowLabs and custom models developed using VaderSentiment analysis. All project code and documentation are accessible via the GitHub link provided on the banner at the bottom of each page.\nBy delving deep into the dynamics of subreddit discussions and utilizing advanced data-processing capabilities, we endeavor to pave the way for more sophisticated methods of identifying and combating misinformation. This research not only sheds light on the patterns of news consumption but also offers a blueprint for enhancing the efficacy of digital platforms as tools for truthful communication.",
    "crumbs": [
      "I. Overview",
      "Introduction"
    ]
  },
  {
    "objectID": "summary.html#about-the-team",
    "href": "summary.html#about-the-team",
    "title": "Introduction",
    "section": "About the Team",
    "text": "About the Team\n\n\n\n\n\n\n\n\n\n\nSharon Chuang Sierra Sikorski Jenny Gong",
    "crumbs": [
      "I. Overview",
      "Introduction"
    ]
  },
  {
    "objectID": "goals.html",
    "href": "goals.html",
    "title": "Business Goals",
    "section": "",
    "text": "Business Goal -1 : Determine which day people use Reddit most frequently.\n\n Technical proposal:  To find this, we will extract the day from the DateTime variable. We will then group by the new day variable and count the group by data. We will create a bar graph of the counts to visualize the findings. This will be conducted for both comments and submissions.\n\n\n \n\n Business Goal - 2 : Determine the frequency of fake news identifiers.\n\nTechnical proposal : We will extract the year from the DateTime variable. Using this, we will group by year and count the grouped data. We will visualize our findings in a table that includes comments and submissions.\n\n\n \n\n Business Goal - 3 : Determine which fake news identifier is used the most. \n\nTechnical Proposal : First, we will utilize regex to match on different fake news identifiers like bullshit, propaganda, or fake news. Using this, we will create a fake news indicator column that shows if the phrase was present in the comment. Then, we will group by the indicator column and count the groups. We will visualize our findings in a table.\n\n\n \n\nBusiness Goal - 4 : Who are those most active accounts in these news subreddits? \n\nTechnical Proposal : To determine the most active accounts in the news subreddits, we will identify the top 10 users with the highest number of posts. We will extract the usernames from the dataset and group by each user to count their posts. Then, we will compare the number of posts made by these top users in both subreddits. Finally, we will present the findings in a bar chart.\n\n\n \n\nBusiness Goal - 5 : Determine the frequency and the accumulated scores of news sources. \n\nTechnical Proposal:  In this analysis, our objective is to comprehend the frequency and cumulative scores associated with different news sources. Initially, we’ll extract the “domain” information from the URL of each post. Subsequently, we’ll aggregate the data based on these sources, computing both the frequency of posts and summarizing the scores attributed to each source.\n\n\n \n\n\n\n\nBusiness Goal - 6 : Determine the news topics that are interacted with the most.  Among all types of news, which category do people concern about the most? Is it related to war, disaster, food crisis or something else? \n\nTechnical Proposal: Use topic modeling to create different broad news categories. Group by those categories and analyze the comments.\n\n\n \n\nBusiness Goal - 7 :  Determine what topics are perceived as fake news. \n\nTechnical Proposal : Use topic modeling created above. Perform sentiment analysis on the comments. Comments with negative sentiment and containing misinformation identifiers will be misinformation. Topics with more of these comments will be perceived as fake news.\n\n\n \n\nBusiness Goal - 8 :  Find this difference in perceived misinformation from 2021 (post-election year) to 2023 (pre-election year). \n\nTechnical Proposal : We will use the combined sentiment analysis and misinformation indicator to identify comments with negative sentiment and contain a misinformation indicator. We will group by this new variable and count the grouped data. Since there are significantly fewer comments in 2023, we will then take the rate of misinformation comments to compare the two.\n\n\n \n\n\n\n\nBusiness Goal - 8 :  Which articles might be perceived as misinformation, or labeled by users as fake news? \n\nTechnical Proposal : We employ a supervised machine learning model to predict articles that are prone to be perceived as misinformation. Specifically, we’ll utilize techniques such as logistic regression to add precision and robustness to the classification process. By analyzing various attributes of article titles, we’ll identify patterns that indicate potential fake news. This comprehensive approach will not only highlight the types of titles more likely to be labeled as fake news but also pinpoint the key features crucial for classification accuracy. This will enhance our understanding of misinformation dynamics and improve our ability to detect and mitigate its spread.\n\n\n \n\nBusiness Goal - 9 :  During a public health emergency like COVID-19, how does it impact people’s activity on the New and WorldNews subreddits? \n\nTechnical Proposal : Our team aims to quantitatively assess how these events have influenced user activity on the News and WorldNews subreddits. We plan to employ linear regression and correlation analysis techniques to explore relationships between COVID-19 case data and the volume of comments, particularly those identified as misinformation. By integrating comprehensive COVID-19 statistics with misinformation comment counts, we intend to uncover patterns that may reveal shifts in user engagement and misinformation dissemination during key phases of the pandemic. This analysis will not only provide a clearer understanding of the pandemic’s effect on information exchange in these forums but also help identify misinformation trends that could inform future public health communication strategies.\n\n\n \n\nBusiness Goal - 10 :  Among political topics, is there any correlation? Are certain topics mentioned together more frequently than others?\n\nTechnical Proposal : We will employ K-means clustering to analyze relationships between different political topics. After conducting topic modeling, we will filter out topics irrelevant to politics, ensuring our focus remains on pertinent themes. This analysis aims to uncover the connections and interactions among various political themes, providing insights into how these topics coalesce and influence public discourse. Understanding these relationships will offer valuable context for policymakers and analysts seeking to interpret trends and patterns within political discussions.\n\n\n \n\n\n\n\nBusiness Goal - 11 : Among political topics, is there any correlation? Are certain topics mentioned together more frequently than others?\n\nTechnical Proposal: We will conduct a network analysis to explore the relationships between different political topics. This analysis will help us understand the connections and interactions among various political themes, providing insights into how they are interconnected or influence each other within the broader political landscape.",
    "crumbs": [
      "I. Overview",
      "Business Goals"
    ]
  },
  {
    "objectID": "goals.html#exploratory-data-analysis-goals",
    "href": "goals.html#exploratory-data-analysis-goals",
    "title": "Business Goals",
    "section": "",
    "text": "Business Goal -1 : Determine which day people use Reddit most frequently.\n\n Technical proposal:  To find this, we will extract the day from the DateTime variable. We will then group by the new day variable and count the group by data. We will create a bar graph of the counts to visualize the findings. This will be conducted for both comments and submissions.\n\n\n \n\n Business Goal - 2 : Determine the frequency of fake news identifiers.\n\nTechnical proposal : We will extract the year from the DateTime variable. Using this, we will group by year and count the grouped data. We will visualize our findings in a table that includes comments and submissions.\n\n\n \n\n Business Goal - 3 : Determine which fake news identifier is used the most. \n\nTechnical Proposal : First, we will utilize regex to match on different fake news identifiers like bullshit, propaganda, or fake news. Using this, we will create a fake news indicator column that shows if the phrase was present in the comment. Then, we will group by the indicator column and count the groups. We will visualize our findings in a table.\n\n\n \n\nBusiness Goal - 4 : Who are those most active accounts in these news subreddits? \n\nTechnical Proposal : To determine the most active accounts in the news subreddits, we will identify the top 10 users with the highest number of posts. We will extract the usernames from the dataset and group by each user to count their posts. Then, we will compare the number of posts made by these top users in both subreddits. Finally, we will present the findings in a bar chart.\n\n\n \n\nBusiness Goal - 5 : Determine the frequency and the accumulated scores of news sources. \n\nTechnical Proposal:  In this analysis, our objective is to comprehend the frequency and cumulative scores associated with different news sources. Initially, we’ll extract the “domain” information from the URL of each post. Subsequently, we’ll aggregate the data based on these sources, computing both the frequency of posts and summarizing the scores attributed to each source.",
    "crumbs": [
      "I. Overview",
      "Business Goals"
    ]
  },
  {
    "objectID": "goals.html#natural-language-processing-goals",
    "href": "goals.html#natural-language-processing-goals",
    "title": "Business Goals",
    "section": "",
    "text": "Business Goal - 6 : Determine the news topics that are interacted with the most.  Among all types of news, which category do people concern about the most? Is it related to war, disaster, food crisis or something else? \n\nTechnical Proposal: Use topic modeling to create different broad news categories. Group by those categories and analyze the comments.\n\n\n \n\nBusiness Goal - 7 :  Determine what topics are perceived as fake news. \n\nTechnical Proposal : Use topic modeling created above. Perform sentiment analysis on the comments. Comments with negative sentiment and containing misinformation identifiers will be misinformation. Topics with more of these comments will be perceived as fake news.\n\n\n \n\nBusiness Goal - 8 :  Find this difference in perceived misinformation from 2021 (post-election year) to 2023 (pre-election year). \n\nTechnical Proposal : We will use the combined sentiment analysis and misinformation indicator to identify comments with negative sentiment and contain a misinformation indicator. We will group by this new variable and count the grouped data. Since there are significantly fewer comments in 2023, we will then take the rate of misinformation comments to compare the two.",
    "crumbs": [
      "I. Overview",
      "Business Goals"
    ]
  },
  {
    "objectID": "goals.html#machine-learning-goals",
    "href": "goals.html#machine-learning-goals",
    "title": "Business Goals",
    "section": "",
    "text": "Business Goal - 8 :  Which articles might be perceived as misinformation, or labeled by users as fake news? \n\nTechnical Proposal : We employ a supervised machine learning model to predict articles that are prone to be perceived as misinformation. Specifically, we’ll utilize techniques such as logistic regression to add precision and robustness to the classification process. By analyzing various attributes of article titles, we’ll identify patterns that indicate potential fake news. This comprehensive approach will not only highlight the types of titles more likely to be labeled as fake news but also pinpoint the key features crucial for classification accuracy. This will enhance our understanding of misinformation dynamics and improve our ability to detect and mitigate its spread.\n\n\n \n\nBusiness Goal - 9 :  During a public health emergency like COVID-19, how does it impact people’s activity on the New and WorldNews subreddits? \n\nTechnical Proposal : Our team aims to quantitatively assess how these events have influenced user activity on the News and WorldNews subreddits. We plan to employ linear regression and correlation analysis techniques to explore relationships between COVID-19 case data and the volume of comments, particularly those identified as misinformation. By integrating comprehensive COVID-19 statistics with misinformation comment counts, we intend to uncover patterns that may reveal shifts in user engagement and misinformation dissemination during key phases of the pandemic. This analysis will not only provide a clearer understanding of the pandemic’s effect on information exchange in these forums but also help identify misinformation trends that could inform future public health communication strategies.\n\n\n \n\nBusiness Goal - 10 :  Among political topics, is there any correlation? Are certain topics mentioned together more frequently than others?\n\nTechnical Proposal : We will employ K-means clustering to analyze relationships between different political topics. After conducting topic modeling, we will filter out topics irrelevant to politics, ensuring our focus remains on pertinent themes. This analysis aims to uncover the connections and interactions among various political themes, providing insights into how these topics coalesce and influence public discourse. Understanding these relationships will offer valuable context for policymakers and analysts seeking to interpret trends and patterns within political discussions.",
    "crumbs": [
      "I. Overview",
      "Business Goals"
    ]
  },
  {
    "objectID": "goals.html#future-goals",
    "href": "goals.html#future-goals",
    "title": "Business Goals",
    "section": "",
    "text": "Business Goal - 11 : Among political topics, is there any correlation? Are certain topics mentioned together more frequently than others?\n\nTechnical Proposal: We will conduct a network analysis to explore the relationships between different political topics. This analysis will help us understand the connections and interactions among various political themes, providing insights into how they are interconnected or influence each other within the broader political landscape.",
    "crumbs": [
      "I. Overview",
      "Business Goals"
    ]
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "In the  Submissions  dataset :\n\n\n\nAuthor : Denotes who posted the submission.\n\n\nCreated_utc : Displays the date and time a submission was posted. Time is in coordinated universal time.\n\n\n ID : The unique identifier of each Submission.\n\n\nSource : Contains the source extracted form news’ URL.\n\n\nScore : The Karma score awarded to each post\n\n\nSubreddit : This shows which subreddit the submission was posted to.\n\n\nIn the  Comments  dataset :\n\n\n\nAuthor : The user who posted the comment.\n\n\nCreated_utc : Displays the date and time the comment was posted. Time is in coordinated universal time\n\n\nbody : The text in the comment.\n\n\n ID : The unique identifier of each comment.\n\n\n Score : Contains the karma score (number of up and down votes) a submission gets.\n\n\nWe created a misinformation classifier from the body variable to show if the comment contained a misinformation phrase like “fake news” or “propaganda.” We also used the created_utc variable to get the year and day of the week a submission or comment was made.\nExternal data: We plan on using vaccination data from Google linked below. Google COVID-19 Vaccination Data",
    "crumbs": [
      "II. Exploratory Data Analysis",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#preprocessing-and-data-dictionary",
    "href": "eda.html#preprocessing-and-data-dictionary",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "In the  Submissions  dataset :\n\n\n\nAuthor : Denotes who posted the submission.\n\n\nCreated_utc : Displays the date and time a submission was posted. Time is in coordinated universal time.\n\n\n ID : The unique identifier of each Submission.\n\n\nSource : Contains the source extracted form news’ URL.\n\n\nScore : The Karma score awarded to each post\n\n\nSubreddit : This shows which subreddit the submission was posted to.\n\n\nIn the  Comments  dataset :\n\n\n\nAuthor : The user who posted the comment.\n\n\nCreated_utc : Displays the date and time the comment was posted. Time is in coordinated universal time\n\n\nbody : The text in the comment.\n\n\n ID : The unique identifier of each comment.\n\n\n Score : Contains the karma score (number of up and down votes) a submission gets.\n\n\nWe created a misinformation classifier from the body variable to show if the comment contained a misinformation phrase like “fake news” or “propaganda.” We also used the created_utc variable to get the year and day of the week a submission or comment was made.\nExternal data: We plan on using vaccination data from Google linked below. Google COVID-19 Vaccination Data",
    "crumbs": [
      "II. Exploratory Data Analysis",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#analyzing-weekly-activity-trends",
    "href": "eda.html#analyzing-weekly-activity-trends",
    "title": "Exploratory Data Analysis",
    "section": "Analyzing Weekly Activity Trends",
    "text": "Analyzing Weekly Activity Trends\nIn this section, we delve into understanding the patterns of user engagement on Reddit. We begin by examining the distribution of activity across different days of the week, extracted from the DateTime variable. By grouping the data based on these days, we quantify the frequency of user interactions, both in terms of comments and submissions. The insights derived from this analysis are visually represented through a bar graph in Figure 2.1 and Figure 2.2.\n\n\n                                                \n\n\n Figure 2.1: Number of submissions per day of the week from 2021-2023   As shown in Figure 2.1, the trend for submissions exhibits a significant dip during the weekends. This suggests that users are less likely to initiate new threads or topics on Saturday and Sunday.A potential factor contributing to this weekend slump could be the downtime in news cycles, as journalists and news outlets typically slow down on these days.Interestingly, Thursday is the busiest day for submissions, contrary to the intuitive expectation that Monday would start the week with a surge.\n\n\n                                                \n\n\n Figure 2.2: Number of comments per day of the week from 2021-2023   Figure 2.2 depicts a different dynamic for comments, with activity gradually increasing from Monday, reaching a zenith on Thursday. This progressive increase could indicate users’ growing engagement with content as the week unfolds. Despite both submissions and comments peaking on Thursday, only comments display a steady climb throughout the weekdays.  When we synthesize the data from submissions and comments, a compelling narrative about user engagement emerges. Thursday stands out as a pinnacle of activity for Reddit, with both submissions and comments reaching their highest levels. This indicates that Thursdays are not just about new content being created but also about the peak in interactions with existing threads. The pattern across the week shows more engagement with ongoing discussions rather than starting new ones, especially as the week progresses.",
    "crumbs": [
      "II. Exploratory Data Analysis",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#analyzing-post-frequency-by-year",
    "href": "eda.html#analyzing-post-frequency-by-year",
    "title": "Exploratory Data Analysis",
    "section": "Analyzing Post Frequency by Year",
    "text": "Analyzing Post Frequency by Year\nIn this analysis, we investigate the disparity in the number of posts across different years. Utilizing the DateTime variable, we extract the year component to group the data accordingly. The findings are presented in two separate tables: Table 2.1 for submissions and Table 2.2 for comments.\n\n\n╒════════╤═════════╕\n│   year │ count   │\n╞════════╪═════════╡\n│   2021 │ 998,335 │\n│   2022 │ 732,024 │\n│   2023 │ 116,779 │\n╘════════╧═════════╛\n\n\n Table 2.1: Submissions Per Year  \n\n\n╒════════╤════════════╕\n│   year │ count      │\n╞════════╪════════════╡\n│   2022 │ 27,272,489 │\n│   2021 │ 19,060,400 │\n│   2023 │ 3,637,011  │\n╘════════╧════════════╛\n\n\n Table 2.2: Comments Per Year   The analysis of post frequency by year reveals intriguing trends. While there is a notable decline in submissions from 2021 to 2023, the number of comments exhibits a contrasting pattern, with a significant increase observed from 2021 to 2022 followed by a decline in 2023. This divergence suggests a potential shift in user behavior towards increased engagement with existing content rather than generating new posts. Further investigation into the underlying factors influencing this trend could provide valuable insights into evolving user preferences and platform dynamics.",
    "crumbs": [
      "II. Exploratory Data Analysis",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#assessing-percentage-of-comments-with-fake-news-indicators",
    "href": "eda.html#assessing-percentage-of-comments-with-fake-news-indicators",
    "title": "Exploratory Data Analysis",
    "section": "Assessing Percentage of Comments with Fake News Indicators",
    "text": "Assessing Percentage of Comments with Fake News Indicators\nTo evaluate the prevalence of fake news indicators in comments, we employed regex to detect phrases such as “fake news,” “bullshit,” or “propaganda.” Subsequently, a fake news indicator column was created to denote the presence of these phrases in comments. Grouping the data by this indicator column, we tallied the counts and visualized the findings in a chart. Among the total comments analyzed, 407,621  were flagged as containing fake news indicators, while the majority, comprising 49,562,279 comments, were deemed free from such indicators. This signifies that approximately 0.8% of comments were identified as potentially containing fake news elements. Further exploration into the context and implications of these comments could offer valuable insights into the dissemination of misinformation within online communities.\n\n\n                                                \n\n\n Figure 2.3: Comments containing Misinformation Indicators from 2021-2023",
    "crumbs": [
      "II. Exploratory Data Analysis",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#analyzing-user-activity",
    "href": "eda.html#analyzing-user-activity",
    "title": "Exploratory Data Analysis",
    "section": "Analyzing User Activity",
    "text": "Analyzing User Activity\nAs we delve into the dynamics of user interactions within the news-centric communities on Reddit, we encounter some intriguing patterns. The subreddit under examination boasts 31.5 million subscribers. However, a closer inspection reveals that in the past year, approximately 27,000 unique users have made submissions, and around 1.2 million have commented.\n\n\n\n\n\n\n\n\n\n Figure 2.4: Top 10 Users Post Comparison for News and World News Subreddits  \n\n\n╒══════════════════════╤════════════════════╕\n│ author               │   submission_count │\n╞══════════════════════╪════════════════════╡\n│ rajacreator          │              68724 │\n│ popularnewsindia     │              30485 │\n│ newsnationglobal     │              17184 │\n│ roknonline           │              15960 │\n│ Som2ny-Official      │              14037 │\n│ The_Dispatch         │              13240 │\n│ First-Situation-1384 │              12811 │\n│ AgeCompetitive4420   │              11398 │\n│ tellygupsgup_redit   │               9747 │\n│ HindustanNewsBharat  │               9550 │\n╘══════════════════════╧════════════════════╛\n\n\n Table 2.4: Top 10 Active User in News Subreddit  \n\n\n╒══════════════════╤════════════════════╕\n│ author           │   submission_count │\n╞══════════════════╪════════════════════╡\n│ harryg888        │              57339 │\n│ theworldnnews    │               5509 │\n│ AdrienSergent    │               5139 │\n│ Infoseven7       │               5107 │\n│ DoremusJessup    │               3999 │\n│ shubhamk1995     │               3532 │\n│ USNEWS01         │               3027 │\n│ Efficient-Ad9226 │               2785 │\n│ radarhitnews     │               2625 │\n│ nkonsontv        │               2577 │\n╘══════════════════╧════════════════════╛\n\n\n Table 2.5: Top 10 Active User in World News Subreddit   The analysis of the two tables indicates a notable prevalence of [deleted] accounts among the top 10 active users in both the “news” and “worldnews” subreddits. Additionally, it reveals a significant disparity in posting frequency between the two categories. Specifically, the top 10 users in the “news” subreddit demonstrate considerably higher posting rates, with counts exceeding 9,000, compared to the “worldnews” subreddit where the counts remain above 2,000. This discrepancy suggests a potential discrepancy in user engagement and interest levels between the two thematic categories, highlighting the diverse participation patterns within the Reddit community across different topics.",
    "crumbs": [
      "II. Exploratory Data Analysis",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#source-analysis",
    "href": "eda.html#source-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Source Analysis",
    "text": "Source Analysis\nIn this extensive analysis, we aim to understand the influence of various news sources within the Reddit ecosystem. To achieve this, we extract the domain information from each submission’s URL and aggregate the data to evaluate two main metrics: the frequency of posts from each news source and the cumulative karma scores these posts have received.\nThis dual-faceted approach enables us to not only see which news sources are most frequently posted but also which ones resonate the most with the Reddit community, as reflected by their karma scores. The results are encapsulated in two distinct tables.\n\n\n\n\n\n\n\n\n\n Figure 2.: Top 10 Users Post Comparison for News and World News Subreddits   To visually represent the frequency of posts by these news sources, we created a word cloud, as shown in the uploaded image. This illustration vividly displays the prominence of certain names, with ‘Reuters’ and ‘BBCWorld’ appearing more prominently, indicating a higher frequency of posts from these sources compared to others like ‘rajacreator’ or ‘tellygupshup’.\n\n\n| source       | frequency   | karma_score   |   score rank |\n|--------------|-------------|---------------|--------------|\n| Reuters      | 319,453     | 327,004       |           43 |\n| rajacreator  | 68,753      | 68,753        |           45 |\n| AP           | 63,886      | 67,112        |           46 |\n| bluzz        | 57,339      | 57,337        |           47 |\n| reuters      | 39,747      | 34,230,489    |            1 |\n| youtube      | 38,580      | 38,472        |           48 |\n| news         | 34,528      | 13,992,972    |           10 |\n| BBCWorld     | 33,884      | 34,733        |           49 |\n| tellygupshup | 31,600      | 31,596        |           50 |\n| popularnews  | 30,492      | 30,491        |           51 |\n\n\n Table 2.5: Top 10 Sources by Frequency  \nTable 2.5 lists the top 10 sources by the frequency of posts. Reuters takes the lead in the number of posts, with a significant presence on the platform. However, when we consider the karma scores, the same source shows a dramatic contrast in its two entries, one with a high karma score and another much lower, indicating perhaps a discrepancy in the content’s reception or the presence of multiple accounts associated with the source.\n\n\n| source          | frequency   | karma_score   |   frequency rank |\n|-----------------|-------------|---------------|------------------|\n| reuters         | 39,747      | 34,230,489    |                5 |\n| cnn             | 20,535      | 26,569,966    |               13 |\n| theguardian     | 24,648      | 24,930,338    |               12 |\n| apnews          | 20,407      | 23,547,749    |               14 |\n| nbcnews         | 10,907      | 20,146,178    |               21 |\n| cnbc            | 7,391       | 16,038,330    |               31 |\n| businessinsider | 6,772       | 15,497,047    |               33 |\n| bbc             | 29,034      | 15,373,615    |               11 |\n| cbsnews         | 5,514       | 14,944,943    |               41 |\n| news            | 34,528      | 13,992,972    |                7 |\n\n\n Table 2.6: Top 10 Sources by Karma score   Table 2.6, on the other hand, ranks the top 10 sources by the karma score. This table reveals that while Reuters may not have the highest posting frequency among the top sources, it garners the highest cumulative karma, suggesting a strong engagement from the Reddit community with the content provided by this source.  These analyses not only highlight the most active news sources but also offer insights into the quality of engagement that different sources inspire among Redditors. Such data is invaluable for understanding the landscape of news consumption and dissemination on one of the world’s largest social platforms.",
    "crumbs": [
      "II. Exploratory Data Analysis",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Conclusion",
    "section": "",
    "text": "This project has examined perceived fake news on Reddit, specifically looking at the differences between topics. Though this project had its challenges, specifically with the overwhelming amount of reddit data we collected, we were able to find some interesting results. Below we have included graph 3.1 which shows the topic breakdown and 3.2 which shows the misinformation counts for each topic.",
    "crumbs": [
      "V. Results",
      "Conclusion"
    ]
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "Natural Language Processing Analysis",
    "section": "",
    "text": "The data preprocessing pipeline involves several steps to prepare text data for topic modeling using LDA (Latent Dirichlet Allocation) and sentiment analysis in PySpark. Here’s a breakdown of the preprocessing steps: Here’s the description of the data preprocessing steps before applying the LDA model:\n\nText Cleaning:\n\nThe comments data is cleaned using a custom cleaning function, likely to remove noise, such as special characters, stopwords, and irrelevant content.\nAdditionally, potential typos are addressed by correcting typing errors and adding commonly occurring typos.\n\nIdentifying Misinformation:\n\nA column named ‘misinfo_class’ is added to the dataset, indicating whether the comment contains misinformation-related keywords such as “fake news,” “bullshit,” “propaganda,” etc.\n\nOtherPreparation:\n\nTokenization: The titles of the posts are tokenized using the Tokenizer.\nStopword Removal: Common English stopwords and additional irrelevant terms are removed using the StopWordsRemover class.\nCount Vectorization: The filtered tokens are converted into a numerical vector representation using the CountVectorizer class, limiting the vocabulary size to 5000 and considering only terms with a minimum document frequency of 25.\nInverse Document Frequency (IDF): IDF is applied to the count vectorized features to down-weight the importance of frequent terms.\n\n\nThese preprocessing steps prepare the data for topic modeling using LDA and the following sentiment analysis, ensuring that the text data is properly cleaned, transformed into a suitable format, and mapped to meaningful topics for analysis.",
    "crumbs": [
      "III. Natural Language Processing",
      "Natural Language Processing Analysis"
    ]
  },
  {
    "objectID": "nlp.html#text-pre-processing",
    "href": "nlp.html#text-pre-processing",
    "title": "Natural Language Processing Analysis",
    "section": "",
    "text": "The data preprocessing pipeline involves several steps to prepare text data for topic modeling using LDA (Latent Dirichlet Allocation) and sentiment analysis in PySpark. Here’s a breakdown of the preprocessing steps: Here’s the description of the data preprocessing steps before applying the LDA model:\n\nText Cleaning:\n\nThe comments data is cleaned using a custom cleaning function, likely to remove noise, such as special characters, stopwords, and irrelevant content.\nAdditionally, potential typos are addressed by correcting typing errors and adding commonly occurring typos.\n\nIdentifying Misinformation:\n\nA column named ‘misinfo_class’ is added to the dataset, indicating whether the comment contains misinformation-related keywords such as “fake news,” “bullshit,” “propaganda,” etc.\n\nOtherPreparation:\n\nTokenization: The titles of the posts are tokenized using the Tokenizer.\nStopword Removal: Common English stopwords and additional irrelevant terms are removed using the StopWordsRemover class.\nCount Vectorization: The filtered tokens are converted into a numerical vector representation using the CountVectorizer class, limiting the vocabulary size to 5000 and considering only terms with a minimum document frequency of 25.\nInverse Document Frequency (IDF): IDF is applied to the count vectorized features to down-weight the importance of frequent terms.\n\n\nThese preprocessing steps prepare the data for topic modeling using LDA and the following sentiment analysis, ensuring that the text data is properly cleaned, transformed into a suitable format, and mapped to meaningful topics for analysis.",
    "crumbs": [
      "III. Natural Language Processing",
      "Natural Language Processing Analysis"
    ]
  },
  {
    "objectID": "nlp.html#key-topics-in-submissions",
    "href": "nlp.html#key-topics-in-submissions",
    "title": "Natural Language Processing Analysis",
    "section": "Key Topics in Submissions",
    "text": "Key Topics in Submissions\nWe employed topic modeling through Latent Dirichlet Allocation (LDA) on the cleaned titles of our submissions dataset and obtained 8 topic groups, which covered various aspects of current affairs:\n\n\nrussia&ukraine\n\n\nTop words: ukraine, russian, russia, war, us, putin, minister, eu, media, new\n\n\n\n\nsocial media\n\n\nTop words: video, new, twitter, youtube, covid, elon, musk, news, hong kong\n\n\n\n\ncurrent events\n\n\nTop words: police, man, shooting, capitol, school, trump, black, us, house, woman\n\n\n\n\ntv shows\n\n\nTop words: episode, mtv, splitsvilla, live, th, show, full, getting, june, free\n\n\n\n\ncovid\n\n\nTop words: covid, world, us, queen, cases, global, pandemic, new, India, day\n\n\n\n\nforeign relations\n\n\nTop words: biden, us, president, china, climate, un, court, iran, new, korea\n\n\n\n\nemerging tech\n\n\nTop words: crypto, covid, people, us, keep, bitcoin, variant, new, cut, international\n\n\n\n\ndemographic info\n\n\nTop words: age, family, worth, height, net, indian, biography, wiki, actor, market\n\n\n\n\n\n\n                                                \n\n\n Figure 3.1 : LDA Topic Visualization based on Title \nLooking at Figure 3.1, almost half of the submissions relate to Russia/Ukraine, current events and foreign relations. Surprisingly, covid news is one of the smallest topic categories. This could be because many covid articles were more relevant to other categories like social media or emerging tech.\n\n\n                                                \n\n\n Figure 3.2 : Misinformation Counts by Topic  Looking at Figure 3.2, the largest misinformation count is the Russia Ukraine topic. It has over 120,000, while the second largest topic, current events, has 56,000. TV shows has the least with 16K.",
    "crumbs": [
      "III. Natural Language Processing",
      "Natural Language Processing Analysis"
    ]
  },
  {
    "objectID": "nlp.html#assessing-sentiments-on-comments",
    "href": "nlp.html#assessing-sentiments-on-comments",
    "title": "Natural Language Processing Analysis",
    "section": "Assessing Sentiments on Comments",
    "text": "Assessing Sentiments on Comments\nTo gain further insights into Reddit users’ attitudes toward different topics’ news, we employ sentiment analysis. VADER (Valence Aware Dictionary and sEntiment Reasoner) is utilized for this purpose. VADER is a rule-based sentiment analysis tool that evaluates the sentiment intensity of text. It operates by utilizing a pre-existing lexicon of words and linguistic rules to generate compound sentiment scores ranging from -1 to 1. Positive scores indicate positive sentiment, negative scores indicate negative sentiment, and scores around 0 denote neutrality.\nTo implement sentiment analysis using VADER, we leverage the SentimentIntensityAnalyzer class from the vaderSentiment library. We define a function called vader_sentiment, which accepts text input, computes its VADER score using the polarity_scores method of the analyzer, and returns the compound score.\n\n\n                                                \n\n\n Figure 3.3 : Sentiment Intensity Frequency Across Vader Score Ranges \nThe plot shows a declining trend from the interval [-1.0~-0.9] starting at just over 140,000, dropping sharply until the [0.8~-0.7] interval, then more gradually declining through the [0.7~-0.6] and [0.6~-0.5] intervals. There is a slight increase in frequency at the [0.5~-0.4] interval.\nThis distribution suggests that the most common sentiment scores in the analyzed dataset are strongly negative, as indicated by the higher counts in the negative score ranges. The presence of a minor increase in the last interval might suggest a small concentration of sentiments in that particular range as well.\n\n\n                                                \n\n\n Figure 3.4 : Comparative Sentiment Distribution in Different News and Entertainment Sectors \nIn Figure 3.4, we presents a set of four pie charts titled “Figure 3.4: Vader Score Distribution,” which illustrate the distribution of sentiment scores within four different categories: US politics, economics/Russia&Ukraine, presidential news, and TV shows. Each pie chart is divided into two segments based on a threshold value of -0.8 on the Vader sentiment scale.\nUS Politics: 60.3% of the sentiment scores are above -0.8, suggesting a more positive sentiment, while 39.7% are below -0.8, indicating a more negative sentiment. Economics/Russia&Ukraine: 61% of scores are above -0.8, and 39% are below -0.8, also showing a predominance of more positive sentiment. Presidential News: 59.4% of the scores are above -0.8, while 40.6% are below -0.8. TV Shows: 58.2% are above -0.8, and 41.8% are below -0.8. The color coding is consistent across all charts, with blue representing scores above -0.8 and pink representing scores below -0.8. In all categories, the majority of sentiments are above -0.8, indicating a leaning towards more positive or neutral sentiments overall. The similarity in the distribution across different categories suggests a possible pattern in the sentiment of the content analyzed, with none of the categories showing an overwhelming negative sentiment.",
    "crumbs": [
      "III. Natural Language Processing",
      "Natural Language Processing Analysis"
    ]
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "Machine Learning Analysis",
    "section": "",
    "text": "For our Machine Learning (ML) we had two goals: to predict what articles would be percieved as misindotmation and to examine the network analysis between topics. We used logistic regression to predict if an article would be percieved as misinformation. We used the top 10 words in each topic as dummy variables to assist in our prediction. Though this did not lead to high accuracy, there were some interesting findings which we will detail below.",
    "crumbs": [
      "IV. Machine Learning",
      "Machine Learning Analysis"
    ]
  },
  {
    "objectID": "ml.html#executive-summary",
    "href": "ml.html#executive-summary",
    "title": "Machine Learning Analysis",
    "section": "",
    "text": "For our Machine Learning (ML) we had two goals: to predict what articles would be percieved as misindotmation and to examine the network analysis between topics. We used logistic regression to predict if an article would be percieved as misinformation. We used the top 10 words in each topic as dummy variables to assist in our prediction. Though this did not lead to high accuracy, there were some interesting findings which we will detail below.",
    "crumbs": [
      "IV. Machine Learning",
      "Machine Learning Analysis"
    ]
  },
  {
    "objectID": "ml.html#misinformation-article-prediction",
    "href": "ml.html#misinformation-article-prediction",
    "title": "Machine Learning Analysis",
    "section": "Misinformation article Prediction",
    "text": "Misinformation article Prediction\nWe used the following steps in our Logistic Regression model:\n\n\nGroup by article ID and get count of comments with misinformation indicators\n\n\nAny article with one or more misinformation indicators will be a perceived misinformation article\n\n\nMerge this dataframe with a dataframe containing the id, topic and title of the article\n\n\nDelete duplicates since the previous data frame was each row as a comment under article\n\n\nAdd dummy columns for top words in articles\n\n\nConvert label and topic columns to numerical representation\n\n\nCreate a vector for the topics column\n\n\nCreate a features vector containing the terms columns and topic column\n\n\nUse features vector in the logistic regression model\n\n\n\n\n\n╒══════════════════════╤═════════╕\n│ label                │   count │\n╞══════════════════════╪═════════╡\n│ no perceived misinfo │  367441 │\n│ perceived misinfo    │   76555 │\n╘══════════════════════╧═════════╛\n\n\n Table 4.1 : Articles with Misinformation Counts \nTable 4.1 shows the original data frame with uneven classes. Since articles with misinformation comments were underrepresented, we decided to undersample the articles with no misinformation comments, resulting in the ratio shown in the next table.\n\n\n╒══════════════════════╤═════════╕\n│ label                │   count │\n╞══════════════════════╪═════════╡\n│ no perceived misinfo │   91922 │\n│ perceived misinfo    │   76555 │\n╘══════════════════════╧═════════╛\n\n\n Table 4.2 : Undersampling Counts \n\n\n                                                \n\n\n Figure 4.1 : Articles with Misinformation Counts \nTable 4.2 shows the sample from the entire dataset when undersampling the articles with no comments claiming misinformation. Here, we can see that the classes are almost equa, allowing us to proceed with the logistic regression. \nThe model had an accuracy of about 56% on the test data and 54% on the training dataset. The confusion matrix is shown below.\n\n\n\n\n\n\n\n\n\n Table 4.3 : Confusion Matrix \n\n\n\n\n\n\n\n\n\n Figure 4.3 : Articles with Misinformation Counts \n\n\n╒═══════════════════╤══════════════╤═══════════════╤══════════════════════════════════╤═════════════════════════════════════╤══════════════╤═════════════════════════════════════╕\n│ topic             │   true label │   false label │   incorrectly labeled as misinfo │   incorrectly labeled as no misinfo │   true ratio │   incorrectly labeled misinfo ratio │\n╞═══════════════════╪══════════════╪═══════════════╪══════════════════════════════════╪═════════════════════════════════════╪══════════════╪═════════════════════════════════════╡\n│ emerging tech     │        10128 │          7238 │                             1237 │                                6001 │     0.583209 │                            0.829096 │\n│ social media      │         8416 │          6519 │                             2918 │                                3601 │     0.563509 │                            0.552385 │\n│ current events    │         8745 │          6909 │                             3118 │                                3791 │     0.558643 │                            0.548705 │\n│ covid             │         7345 │          4668 │                              599 │                                4069 │     0.611421 │                            0.87168  │\n│ russia&ukraine    │        25208 │         19796 │                             4751 │                               15045 │     0.560128 │                            0.760002 │\n│ demographic info  │         3962 │          2823 │                              269 │                                2554 │     0.583935 │                            0.904711 │\n│ tv shows          │         3719 │          2196 │                              108 │                                2088 │     0.62874  │                            0.95082  │\n│ foriegn relations │         9750 │          7322 │                             1615 │                                5707 │     0.571111 │                            0.779432 │\n╘═══════════════════╧══════════════╧═══════════════╧══════════════════════════════════╧═════════════════════════════════════╧══════════════╧═════════════════════════════════════╛\n\n\n Table 4.4 : Logistic Regression Table \n\n\n                                                \n\n\n Figure 4.4 : Articles with Misinformation Counts \nOverall, the model was not too accurate. The table below shows analysis by topic. We can see that the model was slightly more accurate at predicting perceived misinformation in COVID news and TV shows. It was slightly less accurate at predicting misinformation in current events. The final column shows the ratio of articles incorrectly labeled as containing no misinformation comments over the total number of falsely predicted articles. It basically shows the ratio of false negatives over false positives and false negatives. We can see the ratio is significantly higher for the topics: tv shows, demographic information, and covid. Foreign events and social media have the smallest ratio at about .55.",
    "crumbs": [
      "IV. Machine Learning",
      "Machine Learning Analysis"
    ]
  },
  {
    "objectID": "ml.html#relation-between-misinformation-and-covid",
    "href": "ml.html#relation-between-misinformation-and-covid",
    "title": "Machine Learning Analysis",
    "section": "Relation between Misinformation and Covid",
    "text": "Relation between Misinformation and Covid\nIn this section of our study, we delve into the relationship between COVID-19 dynamics, such as confirmed cases and vaccination rates, and the prevalence of misinformation within news-related subreddits. Our objective is to determine if significant intersections exist between the progression of the pandemic and the spread of misinformation online.\nTo achieve this, we first analyze the correlation between two sets of data: COVID-19 statistics and counts of misinformation instances. This analysis will help us understand the degree to which these variables move in tandem, if at all. Following the correlation study, we employ linear regression techniques to assess how effectively pandemic-related data can predict the volume of misinformation. This model will allow us to evaluate the predictive power of COVID-19 variables on misinformation trends. Google COVID-19 Vaccination Data\nData Preparetion\nTo conduct a time-series analysis on COVID-19 and Reddit data, we first clean and standardize the date formats to ensure consistency. We then group both datasets by year, week, and month to analyze trends over specific time intervals. Relevant metrics such as case counts for COVID-19 and post counts for Reddit are aggregated accordingly. Finally, we merge the datasets based on their time groupings, preparing them for detailed correlation analysis and regression modeling. This process ensures our data is well-aligned and reliable for examining the relationship between pandemic trends and misinformation on Reddit.\n\n\n                                                \n\n\n Figure 4.5 : Articles with Misinformation Counts \nThis line graph displays the monthly trends of confirmed COVID-19 cases alongside the number of people vaccinated against the virus from January 2020 to January 2023. The red line represents confirmed cases, peaking sharply around mid-2021, indicating significant waves of the pandemic. In contrast, the blue line illustrates the vaccination rates, which show a gradual increase as vaccines become available and then start to plateau as more of the population becomes vaccinated. This visual representation helps identify periods where increased cases might correlate with changes in misinformation spread on platforms such as Reddit.\n\n\n\n\n\n\n\n\n\n Figure 4.6 : Correlation Matrix of Reddit and COVID Data \nThe correlation matrix provides a detailed look at how different variables relate to each other. Notably, there is a very high correlation (almost 1.0) between the counts of comments and misinformation, suggesting that misinformation topics tend to generate significant discussion or engagement on Reddit. Lesser, yet positive correlations exist between misinformation counts and confirmed COVID-19 cases, indicating a potential increase in misinformation during higher transmission periods. Negative correlations between post counts and confirmed cases suggest that higher infection rates might not necessarily lead to an increase in the number of posts, possibly due to overwhelmed users or changing public interest over time. The matrix also shows strong correlations between different vaccination metrics and how these relate to public discourse on Reddit, hinting at the impact of vaccination campaigns on misinformation dynamics.\n\n\n\nMean Squared Error\nR² Score\n\n\n\n\n13161475.249450391\n0.132\n\n\n\nTable 4.5 Linear Regression\nThe evaluation of the linear regression model revealed a Mean Squared Error (MSE) of 13,161,475.249 and an R² Score of 0.1317. These metrics indicate a significant deviation of the model’s predictions from the actual data, with only about 13% of the variance in misinformation counts explained by the model. This suggests that the selected features, primarily COVID-19 statistics like confirmed cases and vaccination rates, provide limited insights into the spread of misinformation.\nThe investigation into the relationship between COVID-19 dynamics and the prevalence of misinformation on Reddit reveals complex interactions influenced by pandemic progression. Despite our rigorous data preparation and analysis, including correlation studies and regression modeling, the predictive power of COVID-19 statistics on misinformation trends remains limited. The linear regression model showed a significant Mean Squared Error (MSE) of 13,161,475.249 and a low R² Score of 0.1317, indicating that only 13% of the variance in misinformation counts could be explained through COVID-19 variables such as confirmed cases and vaccination rates. This underscores the need for integrating more comprehensive data and possibly more sophisticated analytical techniques to enhance the accuracy and effectiveness of predictions related to misinformation spread during health crises.",
    "crumbs": [
      "IV. Machine Learning",
      "Machine Learning Analysis"
    ]
  },
  {
    "objectID": "ml.html#clustering",
    "href": "ml.html#clustering",
    "title": "Machine Learning Analysis",
    "section": "Clustering",
    "text": "Clustering\n\nData Preparation\nWe streamlined the data preparation process for machine learning using a Pipeline that includes several stages: StringIndexer, OneHotEncoder, VectorAssembler, and Normalizer. This approach efficiently transforms and normalizes the data, ensuring it is ready for subsequent analysis and modeling.\n\n\nChoose clustering methods\nChoose several clustering algorithms to compare. Common choices include K-means, Hierarchical clustering, DBSCAN, Gaussian Mixture Models (GMM), and spectral clustering. Each method has its strengths and weaknesses.  Table 4.4 : Comparison of Clustering Methods \n\n\n\n\n\n\n\n\n\nClustering Method\nSuitability for Data Types\nPros\nCons\n\n\n\n\nK-means\nNumerical data, well-separated clusters, clusters with similar sizes\n- Simple and easy to implement- Scales well to large datasets- Works well with spherical clusters\n- Assumes clusters are spherical and equally sized- Sensitive to outliers- Requires predefined number of clusters\n\n\nHierarchical Clustering\nAny data type, small to medium-sized datasets, clusters with irregular shapes\n- No need to specify the number of clusters- Can handle clusters of different sizes and shapes- Provides a dendrogram for visualization\n- Computationally expensive for large datasets- Not suitable for large datasets due to memory constraints- Results can vary based on distance metric and linkage method\n\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise)\nData with noise and outliers, arbitrary-shaped clusters\n- Robust to noise and outliers- Can find clusters of arbitrary shapes and sizes- No need to specify the number of clusters\n- Sensitivity to the epsilon and minPts parameters- Not suitable for high-dimensional data- Difficulty handling clusters of varying densities\n\n\nGaussian Mixture Models (GMM)\nData with overlapping clusters, probabilistic cluster assignments\n- Can capture complex cluster shapes and overlapping clusters- Provides probabilistic cluster assignments- Flexible in terms of cluster covariance\n- Sensitive to initialization and local optima- Computationally expensive for large datasets and high-dimensional data- May converge to poor solutions for small datasets\n\n\nSpectral Clustering\nNon-linear data, graph-based data, clusters with arbitrary shapes\n- Can find clusters of arbitrary shapes and sizes- Effective for non-linearly separable data- Robust to noise and outliers\n- Requires tuning of parameters such as the number of clusters and affinity matrix- Computationally expensive for large datasets- Difficulty handling large number of clusters\n\n\n\n\n\n\n\n\n\n\n\n\n Figure 4.2 : PCA of Clustering Results Based on News Titles \n\nDiversity and Overlap: The visualization shows both diversity and overlap among clusters. Clusters 1, 3, and 4 show more specific grouping characteristics that might correspond to unique news themes or linguistic styles, while Cluster 0 and 2 indicate broader or more common themes.\nPrincipal Components as Features: The PCA components are likely capturing underlying patterns in the usage of language across different news titles, which could be reflective of topic prevalence, sentiment, stylistic elements, or other latent features.\nUsefulness for Further Analysis: This kind of visualization is particularly useful for understanding how well the PCA and clustering algorithm have managed to discern and categorize the inherent structures in the data. It helps in deciding if further tuning is necessary, whether additional features should be considered, or if a different number of clusters might be more appropriate.",
    "crumbs": [
      "IV. Machine Learning",
      "Machine Learning Analysis"
    ]
  }
]