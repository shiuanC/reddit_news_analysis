# Exploratory Data Analysis 

```{python}
#| echo: false
#| warning: false 

### Importing require libraries 
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd
import numpy as np
import datetime

from IPython.display import display, Markdown
from IPython.display import Image

```
 
## Preprocessing and Data Dictionary 

We assessed the basic specifications of the dataset, removed duplicates and anomalies and dropped undesired columns to finally get <b>170,144</b> submissions and <b>18,548,934</b> comments in the respective datasets. The variables of interest after preprocessing the datasets are listed below: 

In the <b> Submissions </b> dataset :  
<ul> 
    <li> <b>author</b> : The user who created the post. </li>
    <li> <b>created_utc</b> : The time the submission or comment was posted. Was used in the time series analysis section of our project. </li>
    <li> <b>domain</b> : The news site posted in the submission. </li> 
    <li> <b>id </b>: The unique identifier of each post. </li> 
    <li> <b>num_comments</b> : The number of comments under each submission. This may not capture the exact picture as it is dependent on the day the data was retrieved. </li> 
    <li> <b>url </b>: The url associated with the post. Most of the submissions contain this url to the news article. </li> 
    <li> <b>score </b>: The Karma score awarded to each post </li> 
</ul>

In the <b> Comments </b> dataset :  
<ul> 
    <li> <b>author</b> : The user who posted the comment. </li>
    <li> <b>created_utc </b>: The time the submission or comment was posted. Was used in the time series analysis section of our project. </li>
    <li> <b>body</b> : The text in the comment. </li> 
    <li><b> id </b>: The unique identifier of each comment. </li> 
    <li> <b>link_id</b> : The id of the submission under which the comment exists. </li> 
    <li> <b>controversiality</b> : Whether a comment was classified as 'controversial'. </li> 
    <li><b> gilded </b>: Whether the comments have been gilded or awarded. </li> 
    <li><b> distinguished </b>: Whether the comments have been distinguished as moderator. </li> 
    <li><b> score</b> : The Karma score awarded to each post </li> 
</ul>   

During this process, several dummy variables were created to aid in our analysis. A <i>foreign key</i> like variable called <b>submission_id</b> was also created in the comments dataset, that linked any comment to the submission it was made under.  

<a id = "EDA1_DailyEngagement"> </a>

## Analyzing Weekly Activity Trends
In this section, we delve into understanding the patterns of user engagement on Reddit. We begin by examining the distribution of activity across different days of the week, extracted from the DateTime variable. By grouping the data based on these days, we quantify the frequency of user interactions, both in terms of comments and submissions. The insights derived from this analysis are visually represented through a bar graph in Figure 2.1 and Figure 2.2.


```{python}
#| echo: false
#| warning: false 

Image(filename="../data/plots/submissions_day.png")
```
<b> Figure 2.1: Number of submissions per day of the week from 2021-2023  </b> 
<br><br>
As shown in Figure 2.1, the trend for submissions exhibits a significant dip during the weekends. This suggests that users are less likely to initiate new threads or topics on Saturday and Sunday.A potential factor contributing to this weekend slump could be the downtime in news cycles, as journalists and news outlets typically slow down on these days.Interestingly, Thursday is the busiest day for submissions, contrary to the intuitive expectation that Monday would start the week with a surge.

```{python}
#| echo: false
#| warning: false 

Image(filename="../data/plots/day_of_week.png")
```
<b> Figure 2.2: Number of comments per day of the week from 2021-2023  </b> 
<br>
Figure 2.2 depicts a different dynamic for comments, with activity gradually increasing from Monday, reaching a zenith on Thursday. This progressive increase could indicate users' growing engagement with content as the week unfolds. Despite both submissions and comments peaking on Thursday, only comments display a steady climb throughout the weekdays.
<br><br>
When we synthesize the data from submissions and comments, a compelling narrative about user engagement emerges. Thursday stands out as a pinnacle of activity for Reddit, with both submissions and comments reaching their highest levels. This indicates that Thursdays are not just about new content being created but also about the peak in interactions with existing threads. The pattern across the week shows more engagement with ongoing discussions rather than starting new ones, especially as the week progresses.

<a id = "EDA2_DifferencePosts"> </a>

## Analyzing Post Frequency by Year

In this analysis, we investigate the disparity in the number of posts across different years. Utilizing the DateTime variable, we extract the year component to group the data accordingly. The findings are presented in two separate tables: Table 2.1 for submissions and Table 2.2 for comments.

```{python}
#| echo: false
#| warning: false 
# Reading the CSV files into Pandas DataFrames
comments_per_year_df = pd.read_csv('../data/csv/comments_per_year.csv')
submissions_per_year_df = pd.read_csv('../data/csv/submissions_per_year.csv')
print(submissions_per_year_df.to_markdown(tablefmt = "fancy_outline", index = False))

```

<b> Table 2.1: Submissions Per Year </b> 
<br><br>

```{python}
#| echo: false
#| warning: false 

comments_per_year_df['count'] = comments_per_year_df['count'].apply(str)
print(comments_per_year_df.to_markdown(tablefmt = "fancy_outline", index = False))

```
<b> Table 2.2: Comments Per Year </b> 
<br><br>
The analysis of post frequency by year reveals intriguing trends. While there is a notable decline in submissions from 2021 to 2023, the number of comments exhibits a contrasting pattern, with a significant increase observed from 2021 to 2022 followed by a decline in 2023. This divergence suggests a potential shift in user behavior towards increased engagement with existing content rather than generating new posts. Further investigation into the underlying factors influencing this trend could provide valuable insights into evolving user preferences and platform dynamics.


<a id = "EDA3_FakeNews"> </a>

## Assessing Percentage of Comments with Fake News Indicators

To evaluate the prevalence of fake news indicators in comments, we employed regex to detect phrases such as "fake news," "bullshit," or "propaganda." Subsequently, a fake news indicator column was created to denote the presence of these phrases in comments. Grouping the data by this indicator column, we tallied the counts and visualized the findings in a chart. Among the total comments analyzed, 407,621 were flagged as containing fake news indicators, while the majority, comprising 49,562,279 comments, were deemed free from such indicators. This signifies that approximately 0.8% of comments were identified as potentially containing fake news elements. Further exploration into the context and implications of these comments could offer valuable insights into the dissemination of misinformation within online communities. 

```{python}
#| echo: false
#| warning: false 

Image(filename="../data/plots/misinformation.png")
```
<b> Figure 2.3: Comments containing Misinformation Indicators from 2021-2023  </b> 
<br><br>



<a id = "EDA4: Active Users"> </a>

## Analyzing User Activity 

As we delve into the dynamics of user interactions within the news-centric communities on Reddit, we encounter some intriguing patterns. The subreddit under examination boasts 31.5 million subscribers. However, a closer inspection reveals that in the past year, approximately 27,000 unique users have made submissions, and around 1.2 million have commented.


```{python}
#| echo: false
#| warning: false 

# top_users_comparison
# data/plots/top_users_comparison.png
Image(filename="../data/plots/top_users_comparison.png")
```
<b> Figure 2.4: Top 10 Users Post Comparison for News and World News Subreddits </b> 
<br><br>
A minute fraction of these users contribute the bulk of the submissions, which is delineated in Figure 2.4. This chart illustrates a comparison of posting frequency among the top 10 users in both the News and World News subreddits. The disparity is stark; the top-ranked user alone is responsible for more than 60,000 submissions in the News subreddit, significantly overshadowing the rest.


<a id = "EDA5: News Sources"> </a>

## Source Analysis

In this extensive analysis, we aim to understand the influence of various news sources within the Reddit ecosystem. To achieve this, we extract the domain information from each submission's URL and aggregate the data to evaluate two main metrics: the frequency of posts from each news source and the cumulative karma scores these posts have received.

This dual-faceted approach enables us to not only see which news sources are most frequently posted but also which ones resonate the most with the Reddit community, as reflected by their karma scores. The results are encapsulated in two distinct tables.


```{python}
#| echo: false
#| warning: false

## Read images from file 
# data/plots/frequency_wordcloud.png
Image(filename="../data/plots/frequency_wordcloud.png")

```
<b> Figure 2.5: Top 10 Users Post Comparison for News and World News Subreddits </b> 
<br><br>
To visually represent the frequency of posts by these news sources, we created a word cloud, as shown in the uploaded image. This illustration vividly displays the prominence of certain names, with 'Reuters' and 'BBCWorld' appearing more prominently, indicating a higher frequency of posts from these sources compared to others like 'rajacreator' or 'tellygupshup'.

```{python}
#| echo: false
#| warning: false


top100_source_df = pd.read_csv('../data/csv/top100_source.csv')

top100_source_df.sort_values(by='frequency', ascending=False, inplace=True)

# Renaming the 'total_score' column to 'karma_score'
top100_source_df.rename(columns={'total_score': 'karma_score'}, inplace=True)

# Adding a 'rank' column based on the sorted karma_score
top100_source_df['score rank'] = top100_source_df['karma_score'].rank(method='max', ascending=False).astype(int)
top100_source_df['frequency rank'] = top100_source_df['frequency'].rank(method='max', ascending=False).astype(int)

# Convert 'frequency' and 'karma_score' to strings
top100_source_df['frequency'] = top100_source_df['frequency'].apply(str)
top100_source_df['karma_score'] = top100_source_df['karma_score'].apply(str)

# Selecting the top result to display
top_frequency = top100_source_df.head(10)[["source", "frequency", "karma_score", "score rank"]]

print(top_frequency.to_markdown(tablefmt = "fancy_outline", index = False))

```
<b> Table 2.3: Top 10 Sources by Frequency </b> 
<br><br>

Table 2.3 lists the top 10 sources by the frequency of posts. Reuters takes the lead in the number of posts, with a significant presence on the platform. However, when we consider the karma scores, the same source shows a dramatic contrast in its two entries, one with a high karma score and another much lower, indicating perhaps a discrepancy in the content's reception or the presence of multiple accounts associated with the source.

```{python}
#| echo: false
#| warning: false
#| 
top100_source_df['karma_score'] = top100_source_df['karma_score'].apply(int)

top100_source_df.sort_values(by='karma_score', ascending=False, inplace=True)
top100_source_df['karma_score'] = top100_source_df['karma_score'].apply(lambda x: f"{x:,}")

top_score = top100_source_df.head(10)[["source", "frequency", "karma_score", "frequency rank"]]

print(top_score.to_markdown(tablefmt = "fancy_outline", index = False))

```
<b> Table 2.4: Top 10 Sources by Karma score </b> 
<br><br>
Table 2.4, on the other hand, ranks the top 10 sources by the karma score. This table reveals that while Reuters may not have the highest posting frequency among the top sources, it garners the highest cumulative karma, suggesting a strong engagement from the Reddit community with the content provided by this source.
<br><br>
These analyses not only highlight the most active news sources but also offer insights into the quality of engagement that different sources inspire among Redditors. Such data is invaluable for understanding the landscape of news consumption and dissemination on one of the world's largest social platforms.



