# Exploratory Data Analysis 

```{python}
#| echo: false
#| warning: false 

### Importing require libraries 
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd
import numpy as np
import datetime

from IPython.display import display, Markdown, Image

```
 
## Preprocessing and Data Dictionary 



In the <b> Submissions </b> dataset :  
<ul> 
    <li> <b>Author</b> : Denotes who posted the submission. </li>
    <li> <b>Created_utc</b> : Displays the date and time a submission was posted. Time is in coordinated universal time.
 </li>
    <li><b> ID </b>: The unique identifier of each Submission. </li> 
    <li> <b>Source</b> : Contains the source extracted form news' URL. </li> 
    <li> <b>Score </b>: The Karma score awarded to each post </li> 
    <li> <b>Subreddit </b>: This shows which subreddit the submission was posted to. </li> 
   
</ul>

In the <b> Comments </b> dataset :  
<ul> 
    <li> <b>Author</b> : The user who posted the comment. </li>
    <li> <b>Created_utc</b> : Displays the date and time the comment was posted. Time is in coordinated universal time
 </li>
    <li> <b>body</b> : The text in the comment. </li> 
    <li><b> ID </b>: The unique identifier of each comment. </li> 
    <li><b> Score</b> :  Contains the karma score (number of up and down votes) a submission gets. </li> 
</ul>   

We created a misinformation classifier from the body variable to show if the comment contained a misinformation phrase like “fake news” or “propaganda.” We also used the created_utc variable to get the year and day of the week a submission or comment was made.

<b>External data: We plan on using vaccination data from Google linked below.</b> 
<a href="https://health.google.com/covid-19/open-data/raw-data">Google COVID-19 Vaccination Data</a>

<a id = "EDA1_DailyEngagement"> </a>

## Analyzing Weekly Activity Trends
In this section, we delve into understanding the patterns of user engagement on Reddit. We begin by examining the distribution of activity across different days of the week, extracted from the DateTime variable. By grouping the data based on these days, we quantify the frequency of user interactions, both in terms of comments and submissions. The insights derived from this analysis are visually represented through a bar graph in Figure 2.1 and Figure 2.2.


```{python}
#| echo: false
#| warning: false 

# Image(filename="../data/plots/submissions_day.png", unconfined=True)
day_of_week = pd.read_csv('../data/csv/comments_day.csv')
submissions_day = pd.read_csv('../data/csv/submissions_day.csv')
days = day_of_week.merge(submissions_day, on = 'day').sort_values(by= 'day').reset_index(drop=True)
day_replacement = {1: 'Sunday', 2:'Monday', 3: 'Tuesday', 4: 'Wednesday', 5: 'Thursday', 6: 'Friday', 7: 'Saturday'}
days = days.replace(day_replacement)
days = days.rename({'count_x':'comments count', 'count_y': 'submissions count'}, axis = 1)


px.line(
    data_frame = days,
    x = "day",
    y = "submissions count",
  #  opacity = 0.9,
    orientation = "v",
 #   barmode = 'group'
)

```
<b> Figure 2.1: Number of submissions per day of the week from 2021-2023  </b> 
<br><br>
As shown in Figure 2.1, the trend for submissions exhibits a significant dip during the weekends. This suggests that users are less likely to initiate new threads or topics on Saturday and Sunday.A potential factor contributing to this weekend slump could be the downtime in news cycles, as journalists and news outlets typically slow down on these days.Interestingly, Thursday is the busiest day for submissions, contrary to the intuitive expectation that Monday would start the week with a surge.

```{python}
#| echo: false
#| warning: false 

# Image(filename="../data/plots/day_of_week.png", unconfined=True)

px.line(
    data_frame = days,
    x = "day",
    y = "comments count",
  #  opacity = 0.9,
    orientation = "v",
 #   barmode = 'group'
)
```
<b> Figure 2.2: Number of comments per day of the week from 2021-2023  </b> 
<br>
Figure 2.2 depicts a different dynamic for comments, with activity gradually increasing from Monday, reaching a zenith on Thursday. This progressive increase could indicate users' growing engagement with content as the week unfolds. Despite both submissions and comments peaking on Thursday, only comments display a steady climb throughout the weekdays.
<br><br>
When we synthesize the data from submissions and comments, a compelling narrative about user engagement emerges. Thursday stands out as a pinnacle of activity for Reddit, with both submissions and comments reaching their highest levels. This indicates that Thursdays are not just about new content being created but also about the peak in interactions with existing threads. The pattern across the week shows more engagement with ongoing discussions rather than starting new ones, especially as the week progresses.

<a id = "EDA2_DifferencePosts"> </a>

## Analyzing Post Frequency by Year

In this analysis, we investigate the disparity in the number of posts across different years. Utilizing the DateTime variable, we extract the year component to group the data accordingly. The findings are presented in two separate tables: Table 2.1 for submissions and Table 2.2 for comments.

```{python}
#| echo: false
#| warning: false 
# Reading the CSV files into Pandas DataFrames
comments_per_year_df = pd.read_csv('../data/csv/comments_per_year.csv')
submissions_per_year_df = pd.read_csv('../data/csv/submissions_per_year.csv')
# print(submissions_per_year_df.to_markdown(tablefmt = "fancy_outline", index = False, justify="right"))

submissions_per_year_df['count'] = submissions_per_year_df['count'].apply(lambda x: f"{x:,}")
print(submissions_per_year_df.to_markdown(tablefmt = "fancy_outline", index = False))
```

<b> Table 2.1: Submissions Per Year </b> 
<br><br>

```{python}
#| echo: false
#| warning: false 

comments_per_year_df['count'] = comments_per_year_df['count'].apply(lambda x: f"{x:,}")
print(comments_per_year_df.to_markdown(tablefmt = "fancy_outline", index = False))

```
<b> Table 2.2: Comments Per Year </b> 
<br><br>
The analysis of post frequency by year reveals intriguing trends. While there is a notable decline in submissions from 2021 to 2023, the number of comments exhibits a contrasting pattern, with a significant increase observed from 2021 to 2022 followed by a decline in 2023. This divergence suggests a potential shift in user behavior towards increased engagement with existing content rather than generating new posts. Further investigation into the underlying factors influencing this trend could provide valuable insights into evolving user preferences and platform dynamics.


<a id = "EDA3_FakeNews"> </a>

## Assessing Percentage of Comments with Fake News Indicators

To evaluate the prevalence of fake news indicators in comments, we employed regex to detect phrases such as "fake news," "bullshit," or "propaganda." Subsequently, a fake news indicator column was created to denote the presence of these phrases in comments. Grouping the data by this indicator column, we tallied the counts and visualized the findings in a chart. Among the total comments analyzed, <b>407,621 </b> were flagged as containing fake news indicators, while the majority, comprising <b>49,562,279</b> comments, were deemed free from such indicators. This signifies that approximately 0.8% of comments were identified as potentially containing fake news elements. Further exploration into the context and implications of these comments could offer valuable insights into the dissemination of misinformation within online communities. 

```{python}
#| echo: false
#| warning: false 

# Image(filename="../data/plots/misinformation.png")

misinfo = pd.read_csv('../data/csv/misinformation.csv')
fig = px.pie(misinfo, values='count', names='misinfo_class')
fig.show()

```
<b> Figure 2.3: Comments containing Misinformation Indicators from 2021-2023  </b> 
<br><br>



<a id = "EDA4: Active Users"> </a>

## Analyzing User Activity 

As we delve into the dynamics of user interactions within the news-centric communities on Reddit, we encounter some intriguing patterns. The subreddit under examination boasts 31.5 million subscribers. However, a closer inspection reveals that in the past year, approximately 27,000 unique users have made submissions, and around 1.2 million have commented.


```{python}
#| echo: false
#| warning: false 

# top_users_comparison
# data/plots/top_users_comparison.png
Image(filename="../data/plots/top_users_comparison.png")
```
<b> Figure 2.4: Top 10 Users Post Comparison for News and World News Subreddits </b> 
<br><br>

```{python}
#| echo: false
#| warning: false


top_news_authors = pd.read_csv('../data/csv/top_news_authors.csv')
top_news_authors = top_news_authors.iloc[1: , :]
top_worldnews_authors = pd.read_csv('../data/csv/top_worldnews_authors.csv')

top_worldnews_authors = top_worldnews_authors.iloc[1: , :]


print(top_news_authors.to_markdown(tablefmt = "fancy_outline", index = False,  ))

```
<b> Table 2.4: Top 10 Active User in News Subreddit </b> 
<br><br>

```{python}
#| echo: false
#| warning: false

print(top_worldnews_authors.to_markdown(tablefmt = "fancy_outline", index = False,  ))

```
<b> Table 2.5:  Top 10 Active User in World News Subreddit </b> 
<br><br>
The analysis of the two tables indicates a notable prevalence of [deleted] accounts among the top 10 active users in both the "news" and "worldnews" subreddits. Additionally, it reveals a significant disparity in posting frequency between the two categories. Specifically, the top 10 users in the "news" subreddit demonstrate considerably higher posting rates, with counts exceeding 9,000, compared to the "worldnews" subreddit where the counts remain above 2,000. This discrepancy suggests a potential discrepancy in user engagement and interest levels between the two thematic categories, highlighting the diverse participation patterns within the Reddit community across different topics.


<a id = "EDA5: News Sources"> </a>

## Source Analysis

In this extensive analysis, we aim to understand the influence of various news sources within the Reddit ecosystem. To achieve this, we extract the domain information from each submission's URL and aggregate the data to evaluate two main metrics: the frequency of posts from each news source and the cumulative karma scores these posts have received.

This dual-faceted approach enables us to not only see which news sources are most frequently posted but also which ones resonate the most with the Reddit community, as reflected by their karma scores. The results are encapsulated in two distinct tables.


```{python}
#| echo: false
#| warning: false

## Read images from file 
# data/plots/frequency_wordcloud.png
Image(filename="../data/plots/frequency_wordcloud.png")

```
<b> Figure 2.: Top 10 Users Post Comparison for News and World News Subreddits </b> 
<br><br>
To visually represent the frequency of posts by these news sources, we created a word cloud, as shown in the uploaded image. This illustration vividly displays the prominence of certain names, with 'Reuters' and 'BBCWorld' appearing more prominently, indicating a higher frequency of posts from these sources compared to others like 'rajacreator' or 'tellygupshup'.

```{python}
#| echo: false
#| warning: false


top100_source_df = pd.read_csv('../data/csv/top100_source.csv')

top100_source_df.sort_values(by='frequency', ascending=False, inplace=True)

# Renaming the 'total_score' column to 'karma_score'
top100_source_df.rename(columns={'total_score': 'karma_score'}, inplace=True)

# Adding a 'rank' column based on the sorted karma_score
top100_source_df['score rank'] = top100_source_df['karma_score'].rank(method='max', ascending=False).astype(int)
top100_source_df['frequency rank'] = top100_source_df['frequency'].rank(method='max', ascending=False).astype(int)

# Convert 'frequency' and 'karma_score' to strings
top100_source_df['frequency'] = top100_source_df['frequency'].apply(lambda x: f"{x:,}")
top100_source_df['karma_score'] = top100_source_df['karma_score'].apply(lambda x: f"{x:,}")

# Selecting the top result to display
top_frequency = top100_source_df.head(10)[["source", "frequency", "karma_score", "score rank"]]

print(top_frequency.to_markdown(tablefmt = "github", index = False))

```
<b> Table 2.5: Top 10 Sources by Frequency </b> 
<br><br>

Table 2.5 lists the top 10 sources by the frequency of posts. Reuters takes the lead in the number of posts, with a significant presence on the platform. However, when we consider the karma scores, the same source shows a dramatic contrast in its two entries, one with a high karma score and another much lower, indicating perhaps a discrepancy in the content's reception or the presence of multiple accounts associated with the source.

```{python}
#| echo: false
#| warning: false
#| 
top100_source_df['karma_score'] = top100_source_df['karma_score'].str.replace(',', '').apply(int)

top100_source_df.sort_values(by='karma_score', ascending=False, inplace=True)
top100_source_df['karma_score'] = top100_source_df['karma_score'].apply(lambda x: f"{x:,}")

top_score = top100_source_df.head(10)[["source", "frequency", "karma_score", "frequency rank"]]

print(top_score.to_markdown(tablefmt = "github", index = False, ))

```
<b> Table 2.6: Top 10 Sources by Karma score </b> 
<br><br>
Table 2.6, on the other hand, ranks the top 10 sources by the karma score. This table reveals that while Reuters may not have the highest posting frequency among the top sources, it garners the highest cumulative karma, suggesting a strong engagement from the Reddit community with the content provided by this source.
<br><br>
These analyses not only highlight the most active news sources but also offer insights into the quality of engagement that different sources inspire among Redditors. Such data is invaluable for understanding the landscape of news consumption and dissemination on one of the world's largest social platforms.



