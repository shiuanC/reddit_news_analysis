# Natural Language Processing Analysis 
```{python}
#| echo: false
#| warning: false 

### Importing require libraries 
# !pip install pandas
import pandas as pd
import numpy as np
import plotly.express as px
import IPython
from IPython.display import display, Markdown
```

## Text Pre-Processing 
The data preprocessing pipeline involves several steps to prepare text data for topic modeling using LDA (Latent Dirichlet Allocation) and sentiment analysis in PySpark. Here's a breakdown of the preprocessing steps:
Here's the description of the data preprocessing steps before applying the LDA model:


1. **Text Cleaning:**
   - The comments data is cleaned using a custom cleaning function, likely to remove noise, such as special characters, stopwords, and irrelevant content.
   - Additionally, potential typos are addressed by correcting typing errors and adding commonly occurring typos.

2. **Identifying Misinformation:**
   - A column named 'misinfo_class' is added to the dataset, indicating whether the comment contains misinformation-related keywords such as "fake news," "bullshit," "propaganda," etc.

3. **OtherPreparation:**
   - **Tokenization**: The titles of the posts are tokenized using the Tokenizer.
   - **Stopword Removal**: Common English stopwords and additional irrelevant terms are removed using the StopWordsRemover class.
   - **Count Vectorization**: The filtered tokens are converted into a numerical vector representation using the CountVectorizer class, limiting the vocabulary size to 5000 and considering only terms with a minimum document frequency of 25.
   - **Inverse Document Frequency (IDF)**: IDF is applied to the count vectorized features to down-weight the importance of frequent terms.


These preprocessing steps prepare the data for topic modeling using LDA and the following sentiment analysis, ensuring that the text data is properly cleaned, transformed into a suitable format, and mapped to meaningful topics for analysis.


<a id = "topic_modeling"> </a> 

## Key Topics in Submissions 
 
We employed topic modeling through <b>Latent Dirichlet Allocation (LDA)</b> on the cleaned titles of our submissions dataset and obtained 8 topic groups, which covered various aspects of current affairs:
<ol>
  <li>economics/russia&ukraine
    <ul>
      <li><b>Top words</b>: age, ukraine, new, height, indian, amp, crypto, russian, worth, wiki</li>
    </ul>
  </li>
  <li>presidential news
    <ul>
      <li><b>Top words</b>: free, crack, u.s., key, biden, download, first, president, new, say</li>
    </ul>
  </li>
  <li>supreme court/law
    <ul>
      <li><b>Top words</b>: court, first, u.s., new, covid-19, covid, supreme, queen, cases</li>
    </ul>
  </li>
  <li>global politics
    <ul>
      <li><b>Top words</b>: market, global, eu, china, industry, new, u.s., report, amp</li>
    </ul>
  </li>
  <li>us politics
    <ul>
      <li><b>Top words</b>: best, u.s., biden, hong, president, review, 2022, trump</li>
    </ul>
  </li>
  <li>covid/russia&ukraine
    <ul>
      <li><b>Top words</b>: news, covid, covid-19, new, russia, russian, day, ukraine, china</li>
    </ul>
  </li>
  <li>crime/protest
    <ul>
      <li><b>Top words</b>: police, us, man, arrested, death, protests, trump, officer, years, murder</li>
    </ul>
  </li>
  <li>tv shows
    <ul>
      <li><b>Top words</b>: episode, splitsvilla, mtv, show, full, getting, june</li>
    </ul>
  </li>
</ol>


<h3> Figure 3.1 : LDA Topic Visualization based on Title </h3>

```{python}
#| echo: false
#| warning: false 
topics = pd.read_csv('../data/csv/topic_counts.csv')
fig = px.pie(topics, values='count', names='topic')
fig.show()
```

Looking at Figure 3.1, almost half of the submissions relate to COVID-19, Russia/Ukraine, and crime/protest. Surprisingly, US politics is one of the smallest topic categories. This could be because many US news stories were more relevant to other categories like crime/ protest or presidential news.




```{python}
#| echo: false
#| warning: false 
topic_misinfo_true = pd.read_csv('../data/csv/topic_misinfo_true_count.csv')
fig = px.bar(topic_misinfo_true, x='topic', y='count')
fig.show()
```
<b> Figure 3.2 : Misinformation Counts by Topic </b>

<br>
Looking at Figure 3.2, the two largest misinformation comment counts are COVID/Russia and Ukraine war and crime/protest. Both topics have about 90,000 misinformation comments. These are also the two largest categories, as we can see in Figure 3.4. The Supreme Court and Law topic has about 60,000 comments on misinformation. The TV show topic has the least amount od misinformation comments with almost 15,000. 


<a id = "ner"> </a>

## Assessing Sentiments on Comments 
To gain further insights into Reddit users' attitudes toward different topics' news, we employ sentiment analysis. VADER (Valence Aware Dictionary and sEntiment Reasoner) is utilized for this purpose. VADER is a rule-based sentiment analysis tool that evaluates the sentiment intensity of text. It operates by utilizing a pre-existing lexicon of words and linguistic rules to generate compound sentiment scores ranging from -1 to 1. Positive scores indicate positive sentiment, negative scores indicate negative sentiment, and scores around 0 denote neutrality.

To implement sentiment analysis using VADER, we leverage the SentimentIntensityAnalyzer class from the vaderSentiment library. We define a function called vader_sentiment, which accepts text input, computes its VADER score using the polarity_scores method of the analyzer, and returns the compound score.


```{python}
#| echo: false
#| warning: false 
df = pd.read_csv("../data/csv/misinfo_comments_count.csv")

# define bins
bins = np.linspace(-1, 0, 11)
labels = [f"[{bins[i]:.1f}~{bins[i+1]:.1f}]" for i in range(len(bins)-1)]

# divide the data
df['binned'] = pd.cut(df['vader_score'], bins=bins, labels=labels, include_lowest=True)

# aggregate the total count per bin
binned_data = df.groupby('binned').sum().reset_index()

## Plotting
# line chart
fig = px.line(binned_data, x='binned', y='count', markers=True, title='')
fig.update_layout(
    xaxis_title='Vader Score Intervals',
    yaxis_title='Total Count',
    xaxis={'type': 'category', 'tickangle': 0},  # Makes the x-axis labels horizontal
    plot_bgcolor='white',  # Sets background color to white
    showlegend=False,
    width=900,  # Width of the figure in pixels
    height=400  # Height of the figure in pixels
)
fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')
fig.update_xaxes(showgrid=False)  # Hide vertical grid lines

fig.show()

```
<b> Figure 3.3 : Sentiment Intensity Frequency Across Vader Score Ranges </b> 

The plot shows a declining trend from the interval [-1.0~-0.9] starting at just over 140,000, dropping sharply until the [0.8~-0.7] interval, then more gradually declining through the [0.7~-0.6] and [0.6~-0.5] intervals. There is a slight increase in frequency at the [0.5~-0.4] interval.

This distribution suggests that the most common sentiment scores in the analyzed dataset are strongly negative, as indicated by the higher counts in the negative score ranges. The presence of a minor increase in the last interval might suggest a small concentration of sentiments in that particular range as well.

```{python}
#| echo: false
from plotly.subplots import make_subplots
import plotly.graph_objects as go


# load data
df = pd.read_csv("../data/csv/neg_comments_count.csv")

## Plotting
#
df['count_above_neg_0_8'] = df['total_comments'] - df['count_below_neg_0_8']

# Create a subplot of pie charts
colors = ['rgba(255, 99, 132, 0.8)', 'rgba(54, 162, 235, 0.8)']  

# Create a subplot of pie charts without subplot titles
rows = 2
cols = 4
specs = [[{'type': 'pie'} for _ in range(cols)] for _ in range(rows)]
fig = make_subplots(rows=rows, cols=cols, specs=specs, subplot_titles=df['topic'].tolist())

# Adding pie charts
for i, row in df.iterrows():
    fig.add_trace(go.Pie(labels=['Below -0.8', 'Above -0.8'], 
                         values=[row['count_below_neg_0_8'], row['count_above_neg_0_8']],
                         name=row['topic'],
                         textinfo='percent+label',
                         marker_colors=colors), 
                  row=i // cols + 1, col=i % cols + 1)


# Update layout for a dashboard appearance
fig.update_layout(
    title_text="",    
    legend=dict(
        orientation="h",
        x=0.3,
        y=-0.1
    ),
    font=dict(
        size=12,
        color="navy"
    ),
    # Set the height and width of the figure (in pixels)
    height=600,  
    width=1200,  
    showlegend=True,
    margin=dict(t=20, b=20, l=0, r=0)  
)
fig.update_traces(textposition='inside')  

# Show the figure
fig.show()
```
<b> Figure 3.4 : Comparative Sentiment Distribution in Different News and Entertainment Sectors </b>

In Figure 3.4, we presents a set of four pie charts titled "Figure 3.4: Vader Score Distribution," which illustrate the distribution of sentiment scores within four different categories: US politics, economics/Russia&Ukraine, presidential news, and TV shows. Each pie chart is divided into two segments based on a threshold value of -0.8 on the Vader sentiment scale.

US Politics: 60.3% of the sentiment scores are above -0.8, suggesting a more positive sentiment, while 39.7% are below -0.8, indicating a more negative sentiment.
Economics/Russia&Ukraine: 61% of scores are above -0.8, and 39% are below -0.8, also showing a predominance of more positive sentiment.
Presidential News: 59.4% of the scores are above -0.8, while 40.6% are below -0.8.
TV Shows: 58.2% are above -0.8, and 41.8% are below -0.8.
The color coding is consistent across all charts, with blue representing scores above -0.8 and pink representing scores below -0.8. In all categories, the majority of sentiments are above -0.8, indicating a leaning towards more positive or neutral sentiments overall. The similarity in the distribution across different categories suggests a possible pattern in the sentiment of the content analyzed, with none of the categories showing an overwhelming negative sentiment.