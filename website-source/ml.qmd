# Machine Learning Analysis
```{python}
#| echo: false
#| warning: false 

### Importing require libraries 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import ipywidgets as widgets
import IPython

from IPython.display import display, Markdown
from IPython.display import Image
```

## Executive Summary 
For our Machine Learning (ML) we had two goals: to predict what articles would be percieved as misindotmation and to examine the network analysis between topics. We used logistic regression to predict if an article would be percieved as misinformation. We used the top 10 words in each topic as dummy variables to assist in our prediction. Though this did not lead to high accuracy, there were some interesting findings which we will detail below. <br>


<a id = "Prediction"> </a> 

## Misinformation article Prediction

We used the following steps in our Logistic Regression model: 
<ol>
 <li>Group by article ID and get count of comments with misinformation indicators</li>
 <li>Any article with one or more misinformation indicators will be a perceived misinformation article</li>
 <li>Merge this dataframe with a dataframe containing the id, topic and title of the article</li>
 <li>Delete duplicates since the previous data frame was each row as a comment under article</li>
 <li>Add dummy columns for top words in articles </li>
 <li>Convert label and topic columns to numerical representation</li>
 <li>Create a vector for the topics column</li>
 <li>Create a features vector containing the terms columns and topic column</li>
 <li>Use features vector in the logistic regression model</li>
 
<br>

```{python}
#| echo: false
#| warning: false
articles_classified = pd.read_csv('../data/csv/articles_classified.csv')
print(articles_classified.to_markdown(tablefmt = "fancy_outline", index = False))


```
<b> Table 4.1 : Articles with Misinformation Counts </b> 





Table 4.1 shows the original data frame with uneven classes. Since articles with misinformation comments were underrepresented, we decided to undersample the articles with no misinformation comments, resulting in the ratio shown in the next table.



```{python}
#| echo: false
#| warning: false
sample_misinfo_count = pd.read_csv('../data/csv/sample_misinfo_count.csv')
print(sample_misinfo_count.to_markdown(tablefmt = "fancy_outline", index = False))

```
<b> Table 4.2 : Undersampling Counts </b> 

```{python}
#| echo: false
#| warning: false


sample_misinfo_count = pd.read_csv('../data/csv/sample_misinfo_count.csv')
fig = px.pie(sample_misinfo_count, values='count', names='label')
fig.show()

```
<b> Figure 4.1 : Articles with Misinformation Counts </b> 



Table 4.2 shows the sample from the entire dataset when undersampling the articles with no comments claiming misinformation. Here, we can see that the classes are almost equa, allowing us to proceed with the logistic regression. <br>

The model had an accuracy of about 56% on the test data and 54% on the training dataset. The confusion matrix is shown below.



```{python}
#| echo: false
#| warning: false
Image(filename="../data/plots/confusion.png", unconfined=True)

```
<b> Table 4.3 : Confusion Matrix </b> 

```{python}
#| echo: false
#| warning: false
confusion_matrix = np.array([[13008, 3402], [9838, 406611]])


# Plot
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Purples', linewidths=.5)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()



```
<b> Figure 4.3 : Articles with Misinformation Counts </b> 





```{python}
#| echo: false
#| warning: false
ml_analysis_table = pd.read_csv('../data/csv/ml_analysis_table.csv')
print(ml_analysis_table.to_markdown(tablefmt = "fancy_outline", index = False))

```
<b> Table 4.4 : Logistic Regression Table </b> 

```{python}
#| echo: false
#| warning: false

ml_analysis_table = pd.read_csv('../data/csv/ml_analysis_table.csv')
ml_analysis = ml_analysis_table.melt(id_vars='topic', 
                       value_vars = ['incorrectly labeled as misinfo', 'incorrectly labeled as no misinfo'])
fig = px.pie(ml_analysis, values='value',
             names='variable', facet_col='topic', facet_col_wrap = 4)
fig.show()


```
<b> Figure 4.4 : Articles with Misinformation Counts </b> 


Overall, the model was not too accurate. The table below shows analysis by topic. We can see that the model was slightly more accurate at predicting perceived misinformation in COVID news and TV shows. It was slightly less accurate at predicting misinformation in current events. The final column shows the ratio of articles incorrectly labeled as containing no misinformation comments over the total number of falsely predicted articles. It basically shows the ratio of false negatives over false positives and false negatives. We can see the ratio is significantly higher for the topics: tv shows, demographic information, and covid. Foreign events and social media have the smallest ratio at about .55. 

## Relation with the Covid Data
The data we used here is from
<b>External data: We plan on using vaccination data from Google linked below.</b> 

<a href="https://health.google.com/covid-19/open-data/raw-data">Google COVID-19 Vaccination Data</a>

The column we used in her

```{python}
#| echo: false
#| warning: false
import pandas as pd
import plotly.graph_objects as go

# Load data
monthly_covid = pd.read_csv("../data/csv/monthly_covid.csv")

monthly_covid['date'] = pd.to_datetime(monthly_covid['year'].astype(str) + '-' + monthly_covid['month'].astype(str))
monthly_covid.drop(['year', 'month'], axis=1, inplace=True)
monthly_covid.replace({0: None, pd.NA: None}, inplace=True)

# Initialize the plot
fig = go.Figure()

# Add line for each column except 'date'
for column in monthly_covid.columns:
    if column != 'date':
        # Add a trace for each column
        fig.add_trace(go.Scatter(x=monthly_covid['date'], y=monthly_covid[column], mode='lines+markers', name=column))

# Update the layout
fig.update_layout(
    title='',
    xaxis_title='Month',
    yaxis_title='Counts',
    template='plotly_white'
)

# Show the plot
fig.show()

```

<b> Figure 4.4 : Articles with Misinformation Counts </b> 



## Clustering
### Data Preparation
We streamlined the data preparation process for machine learning using a `Pipeline` that includes several stages: `StringIndexer`, `OneHotEncoder`, `VectorAssembler`, and `Normalizer`. This approach efficiently transforms and normalizes the data, ensuring it is ready for subsequent analysis and modeling.


### Choose clustering methods
 Choose several clustering algorithms to compare. Common choices include `K-means`, `Hierarchical clustering`, `DBSCAN`, `Gaussian Mixture Models (GMM`), and `spectral clustering`. Each method has its strengths and weaknesses.
<b> Table 4.4 : Comparison of Clustering Methods </b> 
| Clustering Method | Suitability for Data Types | Pros | Cons |
|--------------------|----------------------------|------|------|
| K-means | Numerical data, well-separated clusters, clusters with similar sizes | - Simple and easy to implement<br>- Scales well to large datasets<br>- Works well with spherical clusters | - Assumes clusters are spherical and equally sized<br>- Sensitive to outliers<br>- Requires predefined number of clusters |
| Hierarchical Clustering | Any data type, small to medium-sized datasets, clusters with irregular shapes | - No need to specify the number of clusters<br>- Can handle clusters of different sizes and shapes<br>- Provides a dendrogram for visualization | - Computationally expensive for large datasets<br>- Not suitable for large datasets due to memory constraints<br>- Results can vary based on distance metric and linkage method |
| DBSCAN (Density-Based Spatial Clustering of Applications with Noise) | Data with noise and outliers, arbitrary-shaped clusters | - Robust to noise and outliers<br>- Can find clusters of arbitrary shapes and sizes<br>- No need to specify the number of clusters | - Sensitivity to the epsilon and minPts parameters<br>- Not suitable for high-dimensional data<br>- Difficulty handling clusters of varying densities |
| Gaussian Mixture Models (GMM) | Data with overlapping clusters, probabilistic cluster assignments | - Can capture complex cluster shapes and overlapping clusters<br>- Provides probabilistic cluster assignments<br>- Flexible in terms of cluster covariance | - Sensitive to initialization and local optima<br>- Computationally expensive for large datasets and high-dimensional data<br>- May converge to poor solutions for small datasets |
| Spectral Clustering | Non-linear data, graph-based data, clusters with arbitrary shapes | - Can find clusters of arbitrary shapes and sizes<br>- Effective for non-linearly separable data<br>- Robust to noise and outliers | - Requires tuning of parameters such as the number of clusters and affinity matrix<br>- Computationally expensive for large datasets<br>- Difficulty handling large number of clusters |



 <b> Figure 4.2 : Confusion Matrix </b> 

```{python}
#| echo: false
#| warning: false
Image(filename="../data/plots/confusion.png", unconfined=True)



```

- Diversity and Overlap: The visualization shows both diversity and overlap among clusters. Clusters 1, 3, and 4 show more specific grouping characteristics that might correspond to unique news themes or linguistic styles, while Cluster 0 and 2 indicate broader or more common themes.
- Principal Components as Features: The PCA components are likely capturing underlying patterns in the usage of language across different news titles, which could be reflective of topic prevalence, sentiment, stylistic elements, or other latent features.
- Usefulness for Further Analysis: This kind of visualization is particularly useful for understanding how well the PCA and clustering algorithm have managed to discern and categorize the inherent structures in the data. It helps in deciding if further tuning is necessary, whether additional features should be considered, or if a different number of clusters might be more appropriate.
