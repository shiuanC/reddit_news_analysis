# Machine Learning Analysis
```{python}
#| echo: false
#| warning: false 

### Importing require libraries 
import pandas as pd
import plotly.express as px
import ipywidgets as widgets
import IPython

from IPython.display import display, Markdown
from IPython.display import Image
```

## Executive Summary 
For our Machine Learning (ML) we had two goals: to predict what articles would be percieved as misindotmation and to examine the network analysis between topics. We used logistic regression to predict if an article would be percieved as misinformation. We used the top 10 words in each topic as dummy variables to assist in our prediction. Though this did not lead to high accuracy, there were some interesting findings which we will detail below. <br>


<a id = "Prediction"> </a> 

## Misinformation article Prediction

We used the following steps in our Logistic Regression model: 
<ol>
 <li>Group by article ID and get count of comments with misinformation indicators</li>
 <li>Any article with one or more misinformation indicators will be a perceived misinformation article</li>
 <li>Merge this dataframe with a dataframe containing the id, topic and title of the article</li>
 <li>Delete duplicates since the previous data frame was each row as a comment under article</li>
 <li>Add dummy columns for top words in articles </li>
 <li>Convert label and topic columns to numerical representation</li>
 <li>Create a vector for the topics column</li>
 <li>Create a features vector containing the terms columns and topic column</li>
 <li>Use features vector in the logistic regression model</li>
 
<br>
<b> Table 4.1 : Articles with Misinformation Counts </b> 

```{python}
#| echo: false
#| warning: false
articles_classified = pd.read_csv('../data/csv/articles_classified.csv')
print(articles_classified.to_markdown(tablefmt = "fancy_outline", index = False))


```

Table 4.1 shows the original data frame with uneven classes. Since articles with misinformation comments were underrepresented, we decided to undersample the articles with no misinformation comments, resulting in the ratio shown in the next table.

<b> Table 4.2 : Undersampling Counts </b> 
```{python}
#| echo: false
#| warning: false
sample_misinfo_count = pd.read_csv('../data/csv/sample_misinfo_count.csv')
print(sample_misinfo_count.to_markdown(tablefmt = "fancy_outline", index = False))

```

Table 4.2 shows the sample from the entire dataset when undersampling the articles with no comments claiming misinformation. Here, we can see that the classes are almost equa, allowing us to proceed with the logistic regression. <br>

The model had an accuracy of about 56% on the test data and 54% on the training dataset. The confusion matrix is shown below.

<b> Table 4.3 : Confusion Matrix </b> 
```{python}
#| echo: false
#| warning: false
Image(filename="../data/plots/confusion.png", unconfined=True)

```

<b> Table 4.4 : Logistic Regression Table </b> 
```{python}
#| echo: false
#| warning: false
ml_analysis_table = pd.read_csv('../data/csv/ml_analysis_table.csv')
print(ml_analysis_table.to_markdown(tablefmt = "fancy_outline", index = False))

```

Overall, the model was not too accurate. The table below shows analysis by topic. We can see that the model was slightly more accurate at predicting perceived misinformation in COVID news and TV shows. It was slightly less accurate at predicting misinformation in current events. The final column shows the ratio of articles incorrectly labeled as containing no misinformation comments over the total number of falsely predicted articles. It basically shows the ratio of false negatives over false positives and false negatives. We can see the ratio is significantly higher for the topics: tv shows, demographic information, and covid. Foreign events and social media have the smallest ratio at about .55. 


```{python}
#| echo: false
#| warning: false


```


 

```{python}
#| echo: false
#| warning: false



```




